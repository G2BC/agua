{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51a16223",
   "metadata": {},
   "source": [
    "# AGUA : Automated Genotyping using Unsupervised Algorithm \n",
    "## training pipeline Python script\n",
    "### Diego Frias, 08.10.2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8db02364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import our three important packages\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import sys\n",
    "import Bio as Bio\n",
    "from Bio import SeqIO\n",
    "from Bio import AlignIO\n",
    "\n",
    "import time as time\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, to_tree\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import dill # for model object handling\n",
    "from Bio import Phylo\n",
    "from Bio.Phylo.TreeConstruction import DistanceMatrix, DistanceTreeConstructor\n",
    "from Bio.Phylo.BaseTree import Clade as Clade\n",
    "from Bio.Phylo.BaseTree import Tree as BioTree\n",
    "from collections import defaultdict\n",
    "\n",
    "from ete3 import Tree, TreeStyle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a8ed63",
   "metadata": {},
   "source": [
    "# FASE 1 - PREPROCESSAMENTO\n",
    "## LEITURA DO DATASET DE TREINAMENTO & IDENTIFICAÇÃO/CODIFICAÇÃO DE CLASSES PRIMÁRIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c92ccfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop codon ordinals  [48, 49, 52]\n"
     ]
    }
   ],
   "source": [
    "base = [16, 4, 1]\n",
    "\n",
    "nucleotides = {\n",
    "\"A\": 0,\n",
    "\"G\": 1,\n",
    "\"C\": 2,\n",
    "\"T\": 3,\n",
    "\"a\": 0,\n",
    "\"g\": 1,\n",
    "\"c\": 2,\n",
    "\"t\": 3,\n",
    "}\n",
    "\n",
    "stops=[\"TAA\",\"TAG\",\"TGA\"]\n",
    "\n",
    "stopnmb=[]\n",
    "\n",
    "for triplet in stops:\n",
    "    codonNumber = 0 # 0 to 63\n",
    "    for p in range(3):\n",
    "          codonNumber += base[p] * nucleotides[triplet[p]]\n",
    "    stopnmb.append(codonNumber)\n",
    "\n",
    "print(\"stop codon ordinals \",stopnmb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "070525a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spike_OM056186\n",
      "spike_OM058922\n",
      "spike_OM059019\n",
      "spike_OM059033\n",
      "spike_OM059246\n",
      "spike_OM065541\n",
      "spike_OM065547\n",
      "spike_OM065569\n",
      "spike_OM065570\n",
      "spike_OM065572\n",
      "spike_OM065582\n",
      "spike_OM065584\n",
      "spike_OM065586\n",
      "spike_OM065603\n",
      "spike_OM065661\n",
      "spike_OM065662\n",
      "spike_OM065665\n",
      "spike_OM065677\n",
      "spike_OM065684\n",
      "spike_OM065682\n",
      "spike_OM065685\n",
      "spike_OM065686\n",
      "spike_OM065688\n",
      "spike_OM065691\n",
      "spike_OM065696\n",
      "spike_OM065709\n",
      "spike_OM065712\n",
      "spike_OM065714\n",
      "spike_OM036675\n",
      "spike_OM036870\n",
      "spike_OM036966\n",
      "spike_OM038376\n",
      "spike_OM116753\n",
      "spike_OM133912\n",
      "spike_OM117210\n",
      "spike_OM117387\n",
      "spike_OM135128\n",
      "spike_OM135237\n",
      "spike_OM136750\n",
      "spike_OM136922\n",
      "spike_OM122019\n",
      "spike_OM125506\n",
      "spike_OM126138\n",
      "spike_OM127121\n",
      "spike_OL919777\n",
      "spike_OM103723\n",
      "spike_OM173411\n",
      "spike_OM173719\n",
      "spike_OM173785\n",
      "spike_OM174072\n",
      "spike_OM155060\n",
      "spike_OM155094\n",
      "spike_OM078510\n",
      "spike_OM078813\n",
      "spike_OM079097\n",
      "spike_OM156007\n",
      "spike_OM156059\n",
      "spike_OM156101\n",
      "spike_OM156219\n",
      "spike_OM156236\n",
      "spike_OM156250\n",
      "spike_OM188423\n",
      "spike_OM096580\n",
      "spike_OM096691\n",
      "spike_OM097184\n",
      "spike_OM098781\n",
      "spike_OM099739\n",
      "spike_OM099777\n",
      "spike_OM229610\n",
      "spike_OM229612\n",
      "spike_OM229648\n",
      "spike_OM229650\n",
      "spike_OM229697\n",
      "spike_OM229711\n",
      "spike_OM229763\n",
      "spike_OM229764\n",
      "spike_OM229804\n",
      "spike_OM229817\n",
      "spike_OM229854\n",
      "spike_OM229881\n",
      "spike_OM229893\n",
      "spike_OM229948\n",
      "spike_OM229954\n",
      "spike_OM229979\n",
      "spike_OM197075\n",
      "spike_OM197156\n",
      "spike_OM232855\n",
      "spike_OM197869\n",
      "spike_OM198452\n",
      "spike_OM236175\n",
      "spike_OM245422\n",
      "spike_OM245453\n",
      "spike_OM245454\n",
      "spike_OM245498\n",
      "spike_OM245504\n",
      "spike_OM245579\n",
      "spike_OM245615\n",
      "spike_OM245673\n",
      "spike_OM245706\n",
      "spike_OM245828\n",
      "spike_OV904299\n",
      "spike_OV904308\n",
      "spike_OV904311\n",
      "spike_OV904312\n",
      "spike_OV904313\n",
      "spike_OV904315\n",
      "spike_OV904316\n",
      "spike_OV904319\n",
      "spike_OV904321\n",
      "spike_OV904326\n",
      "spike_OV904327\n",
      "spike_OV904329\n",
      "spike_OV904332\n",
      "spike_OV904333\n",
      "spike_OV904338\n",
      "spike_OV904343\n",
      "spike_OV905059\n",
      "spike_OV905060\n",
      "spike_OV905061\n",
      "spike_OV905064\n",
      "spike_OV905065\n",
      "spike_OV905066\n",
      "spike_OV905069\n",
      "spike_OV905071\n",
      "spike_OV905072\n",
      "spike_OV905079\n",
      "spike_OV905082\n",
      "spike_OV905085\n",
      "spike_OV905088\n",
      "spike_OV905093\n",
      "spike_OV905098\n",
      "spike_OV905100\n",
      "spike_OV905101\n",
      "spike_OV905102\n",
      "spike_OV905107\n",
      "spike_OV905106\n",
      "spike_OV905108\n",
      "spike_OV905111\n",
      "spike_OV905119\n",
      "spike_OV905120\n",
      "spike_OV905125\n",
      "spike_OV905127\n",
      "spike_OV905129\n",
      "spike_OV905138\n",
      "spike_OV905139\n",
      "spike_OV905140\n",
      "spike_OV905141\n",
      "spike_OV905142\n",
      "spike_OV905143\n",
      "spike_OV905144\n",
      "spike_OV905146\n",
      "spike_OV905153\n",
      "spike_OV905152\n",
      "spike_OV905154\n",
      "spike_OV905157\n",
      "spike_OV905163\n",
      "spike_OV905164\n",
      "spike_OV905162\n",
      "spike_OV905165\n",
      "spike_OV905166\n",
      "spike_OV905169\n",
      "spike_OV905172\n",
      "spike_OV905687\n",
      "spike_OV905689\n",
      "spike_OV905691\n",
      "spike_OV905692\n",
      "spike_OV905696\n",
      "spike_OV905700\n",
      "spike_OV905702\n",
      "spike_OV905703\n",
      "spike_OV905705\n",
      "spike_OV905706\n",
      "spike_OV905707\n",
      "spike_OV905708\n",
      "spike_OV905711\n",
      "spike_OV905715\n",
      "spike_OV905717\n",
      "spike_OV905719\n",
      "spike_OV905722\n",
      "spike_OV905723\n",
      "spike_OV905913\n",
      "spike_OV905923\n",
      "spike_OV905933\n",
      "spike_OV905936\n",
      "spike_OV905940\n",
      "spike_OV905941\n",
      "spike_OV906511\n",
      "spike_OV906515\n",
      "spike_OV906516\n",
      "spike_OV906525\n",
      "spike_OV906526\n",
      "spike_OV906535\n",
      "spike_OV906537\n",
      "spike_OV906539\n",
      "spike_OV906542\n",
      "spike_OV906545\n",
      "spike_OV906548\n",
      "spike_OV906546\n",
      "spike_OV906554\n",
      "spike_OV906558\n",
      "spike_OW390140\n",
      "spike_OW390147\n",
      "spike_OW390192\n",
      "spike_OW390203\n",
      "spike_OW390277\n",
      "spike_OW390299\n",
      "spike_OW390318\n",
      "spike_OW390326\n",
      "spike_OW390348\n",
      "spike_OW390467\n",
      "spike_OW390532\n",
      "spike_OW390668\n",
      "spike_OW390691\n",
      "spike_OW390755\n",
      "spike_OW370259\n",
      "spike_OW390848\n",
      "spike_OW390942\n",
      "spike_OW390993\n",
      "spike_OW391015\n",
      "spike_OW391089\n",
      "spike_OW391095\n",
      "spike_OW391109\n",
      "spike_OW391201\n",
      "spike_OW391565\n",
      "spike_OW391586\n",
      "spike_OW391666\n",
      "spike_OW391712\n",
      "spike_OW391732\n",
      "spike_OW371326\n",
      "spike_OW371330\n",
      "spike_OW391784\n",
      "spike_OW371370\n",
      "spike_OW371388\n",
      "spike_OW371420\n",
      "spike_OW391838\n",
      "spike_OW371464\n",
      "spike_OW391887\n",
      "spike_OW371487\n",
      "spike_OW371526\n",
      "spike_OW371529\n",
      "spike_OW371580\n",
      "spike_OW371586\n",
      "spike_OW371610\n",
      "spike_OW371664\n",
      "spike_OW371675\n",
      "spike_OW371702\n",
      "spike_OW392162\n",
      "spike_OW371767\n",
      "spike_OW371847\n",
      "spike_OW392290\n",
      "spike_OW392555\n",
      "spike_OW372333\n",
      "spike_OW392755\n",
      "spike_OW372367\n",
      "spike_OW372392\n",
      "spike_OW372409\n",
      "spike_OW372473\n",
      "spike_OW372499\n",
      "spike_OW372506\n",
      "spike_OW372515\n",
      "spike_OW392954\n",
      "spike_OW372564\n",
      "spike_OW372565\n",
      "spike_OW372579\n",
      "spike_OW392993\n",
      "spike_OW393005\n",
      "spike_OW372642\n",
      "spike_OW372678\n",
      "spike_OW372719\n",
      "spike_OW372754\n",
      "spike_OW372786\n",
      "spike_OW372812\n",
      "spike_OW372879\n",
      "spike_OW372935\n",
      "spike_OW372951\n",
      "spike_OW393371\n",
      "spike_OW372967\n",
      "spike_OW373035\n",
      "spike_OW373041\n",
      "spike_OW373055\n",
      "spike_OW373069\n",
      "spike_OW393482\n",
      "spike_OW373073\n",
      "spike_OW393499\n",
      "spike_OW373106\n",
      "spike_OW373151\n",
      "spike_OW373162\n",
      "spike_OW393855\n",
      "spike_OW373542\n",
      "spike_OW373545\n",
      "spike_OW394006\n",
      "spike_OW373663\n",
      "spike_OW394053\n",
      "spike_OW394084\n",
      "spike_OW394111\n",
      "spike_OW394135\n",
      "spike_OW373837\n",
      "spike_OW373845\n",
      "spike_OW374028\n",
      "spike_OW374069\n",
      "spike_OU356612\n",
      "spike_OU352177\n",
      "spike_OU352265\n",
      "spike_OU356382\n",
      "spike_OU356422\n",
      "spike_OU356928\n",
      "spike_OU351990\n",
      "spike_OU361508\n",
      "spike_OU360555\n",
      "spike_OU326021\n",
      "spike_OU326139\n",
      "spike_MZ454400\n",
      "spike_OU175769\n",
      "spike_OU175786\n",
      "spike_MZ454575\n",
      "spike_MZ454675\n",
      "spike_MZ454903\n",
      "spike_MZ455098\n",
      "spike_MZ455429\n",
      "spike_OU327146\n",
      "spike_MZ368109\n",
      "spike_MZ368110\n",
      "spike_MZ433214\n",
      "spike_MZ384172\n",
      "spike_MZ433487\n",
      "spike_MZ433495\n",
      "spike_MZ279155\n",
      "spike_MZ279200\n",
      "spike_MZ279404\n",
      "spike_MZ433510\n",
      "spike_MZ345911\n",
      "spike_MZ368496\n",
      "spike_MZ368511\n",
      "spike_MZ368518\n",
      "spike_MZ368520\n",
      "spike_MZ368534\n",
      "spike_MZ349105\n",
      "spike_MZ349110\n",
      "spike_MZ433710\n",
      "spike_MZ371124\n",
      "spike_MZ385757\n",
      "spike_MZ385765\n",
      "spike_MZ405567\n",
      "spike_MZ385779\n",
      "spike_MZ433754\n",
      "spike_MZ371176\n",
      "spike_MZ405683\n",
      "spike_MZ385812\n",
      "spike_MZ371199\n",
      "spike_MZ371231\n",
      "spike_MZ371251\n",
      "spike_MZ385883\n",
      "spike_MZ322030\n",
      "spike_MZ322032\n",
      "spike_MZ322033\n",
      "spike_MZ322035\n",
      "spike_MZ322041\n",
      "spike_MZ322069\n",
      "spike_MZ385932\n",
      "spike_MZ385940\n",
      "spike_MZ371324\n",
      "spike_MZ385962\n",
      "spike_MZ371337\n",
      "spike_MZ323709\n",
      "spike_MZ386035\n",
      "spike_MZ386161\n",
      "spike_MZ434171\n",
      "spike_MZ386202\n",
      "spike_MZ371606\n",
      "spike_MZ386214\n",
      "spike_MZ386222\n",
      "spike_MZ434203\n",
      "spike_MZ349675\n",
      "spike_MZ349682\n",
      "spike_OU066644\n",
      "spike_MZ349688\n",
      "spike_MZ434232\n",
      "spike_MZ386275\n",
      "spike_MZ434249\n",
      "spike_MZ349743\n",
      "spike_MZ434290\n",
      "spike_MZ349749\n",
      "spike_MZ386349\n",
      "spike_MZ349760\n",
      "spike_MZ349763\n",
      "spike_MZ411665\n",
      "spike_MZ411660\n",
      "spike_MZ371792\n",
      "spike_MZ349849\n",
      "spike_MZ386417\n",
      "spike_MZ386446\n",
      "spike_MZ328676\n",
      "spike_MZ411833\n",
      "spike_MZ372040\n",
      "spike_MZ411866\n",
      "spike_MZ388144\n",
      "spike_OU061529\n",
      "spike_MZ328971\n",
      "spike_MZ388222\n",
      "spike_OU151076\n",
      "spike_OU352138\n",
      "spike_OU356670\n",
      "spike_OU352186\n",
      "spike_OU340710\n",
      "spike_OU342235\n",
      "spike_OU342243\n",
      "spike_OU342265\n",
      "spike_OU356466\n",
      "spike_OU342269\n",
      "spike_OU356508\n",
      "spike_OU361574\n",
      "spike_OU139663\n",
      "spike_OU326018\n",
      "spike_OU326028\n",
      "spike_OU124871\n",
      "spike_OU124889\n",
      "spike_OU175732\n",
      "spike_OU175761\n",
      "spike_OU175770\n",
      "spike_OU125176\n",
      "spike_OU175784\n",
      "spike_OU125201\n",
      "spike_OU180312\n",
      "spike_OU140043\n",
      "spike_OU116880\n",
      "spike_OU125871\n",
      "spike_OU125883\n",
      "spike_OU290663\n",
      "spike_OU126114\n",
      "spike_OU176232\n",
      "spike_OU176362\n",
      "spike_OU176390\n",
      "spike_OU117648\n",
      "spike_OU117682\n",
      "spike_OU126622\n",
      "spike_OU126656\n",
      "spike_OU176483\n",
      "spike_MZ344999\n",
      "spike_MZ345001\n",
      "spike_OU093684\n",
      "spike_MZ368225\n",
      "spike_OU106512\n",
      "spike_OU094175\n",
      "spike_OU106981\n",
      "spike_OU106989\n",
      "spike_OU107446\n",
      "spike_OU107562\n",
      "spike_OU066608\n",
      "spike_OU095553\n",
      "spike_OU066774\n",
      "spike_OU072522\n",
      "spike_OU096161\n",
      "spike_OU083191\n",
      "spike_OU083682\n",
      "spike_MZ328979\n",
      "spike_OU151377\n",
      "spike_OU151561\n",
      "spike_OU134599\n",
      "spike_OU134660\n",
      "spike_OU134799\n",
      "spike_OU134852\n",
      "spike_OU134900\n",
      "spike_OU135018\n",
      "spike_OU138792\n",
      "spike_OU123904\n",
      "spike_OU149305\n",
      "spike_OU182580\n",
      "spike_OU149362\n",
      "spike_OU139191\n",
      "spike_OU149471\n",
      "spike_MZ376663\n",
      "spike_OU139260\n",
      "spike_OU135800\n",
      "spike_OU135995\n",
      "spike_OU139382\n",
      "spike_OU149773\n",
      "spike_OU182806\n",
      "spike_OU149846\n",
      "spike_OU124688\n",
      "spike_OU168626\n",
      "spike_OU139634\n",
      "spike_OU149968\n",
      "spike_OU149976\n",
      "spike_OU168731\n",
      "spike_OU171203\n",
      "spike_OU313807\n",
      "spike_OU129298\n",
      "spike_OU161090\n",
      "spike_OU314387\n",
      "spike_MZ341810\n",
      "spike_MZ331077\n",
      "spike_MZ358462\n",
      "spike_MZ394439\n",
      "spike_MZ394580\n",
      "spike_MZ394616\n",
      "spike_MZ394617\n",
      "spike_MZ342413\n",
      "spike_MZ450533\n",
      "spike_MZ378667\n",
      "spike_OU070204\n",
      "number of sequences in file  /home/m_souza/tcc/dataset/dataset_agua/sequencias_treinamento.fasta  =  500\n"
     ]
    }
   ],
   "source": [
    "#import the sequences we will use. These are 16s sequences from GenBank\n",
    "#example: https://www.ncbi.nlm.nih.gov/nuccore/FJ039971.1?report=genbank\n",
    "\n",
    "# fname = \"spike_alingment.fas\" # PROVA DE CONCEITO\n",
    "fname = \"/home/m_souza/tcc/dataset/dataset_agua/sequencias_treinamento.fasta\" # CASO REAL\n",
    "# fname = \"spike_alingment.fas\"\n",
    "# t1 = SeqIO.read(\"sequence1.fasta\", \"fasta\")\n",
    "# t2 = SeqIO.read(\"sequence2.fasta\", \"fasta\")\n",
    "# t3 = SeqIO.read(\"sequence3.fasta\", \"fasta\")\n",
    "# t4 = SeqIO.read(\"sequence4.fasta\", \"fasta\")\n",
    "# t5 = SeqIO.read(\"sequence5.fasta\", \"fasta\")\n",
    "# t6 = SeqIO.read(\"sequence6.fasta\", \"fasta\")\n",
    "# t7 = SeqIO.read(\"sequence7.fasta\", \"fasta\")\n",
    "\n",
    "t=[]\n",
    "with open(fname) as handle:\n",
    "    for values in SeqIO.FastaIO.SimpleFastaParser(handle):\n",
    "        t.append(values)\n",
    "        print(t[-1][0])\n",
    "\n",
    "Nseq=len(t)\n",
    "\n",
    "print(\"number of sequences in file \",fname,\" = \",Nseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee72ed55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of non-redundant accessions in the alignment 500  (# replicated:  0 )\n"
     ]
    }
   ],
   "source": [
    "# Open the alignment file AGAIN BUT NOW as a MultipleSeqAlignment object\n",
    "\n",
    "with open(fname,\"r\") as aln:\n",
    "    alignment = AlignIO.read(aln,\"fasta\")\n",
    "\n",
    "idlst=[]\n",
    "sequences=[]\n",
    "labels=[]\n",
    "ctr=0 # conta não redundantes\n",
    "ctrdup = 0  # conta sequencias duplicadas\n",
    "for strain in alignment:\n",
    "#     print(strain.description)\n",
    "    if 'accn|' in  strain.description:\n",
    "        accn = strain.description.split(\"accn|\")[1]\n",
    "    else:\n",
    "        accn = strain.description\n",
    "\n",
    "    if accn not in idlst:\n",
    "        idlst.append(accn)\n",
    "        labels.append(\"strain\"+str(ctr)) # strain.description # o id só saia cortado e acahava depois que itnha sequncias repetidas\n",
    "        sequences.append(alignment[ctr].seq)\n",
    "        ctr+=1\n",
    "    else:\n",
    "        print(\"WARNING: duplicated description \",accn)\n",
    "        ctrdup+=1\n",
    "\n",
    "Nseqs=len(sequences)\n",
    "print(\"number of non-redundant accessions in the alignment\", Nseqs,\" (# replicated: \",ctrdup,\")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16ae5dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nucleotideToCodon(tripleNucleotides):\n",
    "\n",
    "    codonNumber = 0 # 0 to 63\n",
    "\n",
    "    for p in range(3):\n",
    "        if (\"AGCTagct\".find(tripleNucleotides[p]) != -1):\n",
    "            codonNumber += base[p] * nucleotides[tripleNucleotides[p]]\n",
    "        else:\n",
    "            if tripleNucleotides[p] == \"-\":\n",
    "                codonNumber = 64  # codon com pelo menos 1\n",
    "            elif tripleNucleotides[p] == \"n\" or tripleNucleotides[p] == \"N\":\n",
    "                codonNumber = 65  # codon com pelo menos 1 N\n",
    "            else:\n",
    "                codonNumber = 66 # codon com caractere IUPAC\n",
    "            break\n",
    "\n",
    "    #debb\n",
    "#     if codonNumber in stopnmb:\n",
    "#         print(\"codon \",codonNumber,\" found in sequence\")\n",
    "\n",
    "    return codonNumber\n",
    "\n",
    "def nucleotideSequenceToCodons(sequence):\n",
    "\n",
    "    tripleNucleotidesList = [sequence[i:i + 3] for i in range(0, len(sequence), 3)]\n",
    "\n",
    "    return list(map(lambda triple: nucleotideToCodon(triple), tripleNucleotidesList))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35ae8c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O dataset contem 500 sequências com 1274 resíduos.\n",
      "Transformando as sequências de nucleotídeos em sequências númericas de códons... \n",
      "Transformação concluída em 0.02656986713409424 minutos\n"
     ]
    }
   ],
   "source": [
    "# transformando sequencias de nucleotideos a codons\n",
    "\n",
    "sequenceLength = len(sequences[0])\n",
    "\n",
    "if (sequenceLength % 3 != 0):\n",
    "    print(sequenceLength)\n",
    "    print(\"ERRO: tamanho não Múltiplo de 3\")\n",
    "    exit(1)\n",
    "elif (next((True for seq in sequences if len(seq) > sequenceLength), False)):\n",
    "    print(\"ERRO: as sequências não tem o mesmo tamanho\")\n",
    "else:\n",
    "    nprm = int(sequenceLength / 3) # comprimento das seqs em codons\n",
    "\n",
    "print(f\"O dataset contem {Nseqs} sequências com {nprm} resíduos.\")\n",
    "print(\"Transformando as sequências de nucleotídeos em sequências númericas de códons... \")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "codonSequences = list(map(lambda seq: nucleotideSequenceToCodons(seq), sequences))\n",
    "\n",
    "print(f\"Transformação concluída em {(time.time() - start) / 60} minutos\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "186efd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum codon number:  0 maximum codon number:  66\n"
     ]
    }
   ],
   "source": [
    "# check codon numbering\n",
    "print(\"minimum codon number: \",np.min(codonSequences),\"maximum codon number: \",np.max(codonSequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b281a0ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# filtrando sequências com stop codons TGA,TAG,TAA e tirando do final\n",
    "filtered_codonSeqs=[]\n",
    "filtered_Seqs=[]\n",
    "filtered_SeqID=[]\n",
    "filtered_Label=[]\n",
    "ctr=0\n",
    "laststop=0\n",
    "for seq in codonSequences:  # olho: se assume que todas as sequencias tem ou não tem stopcodon no final\n",
    "    # se não for assim, o corte não será bem feito e teremos sequencias de comprimento diferente\n",
    "    # podendo dar erro mais na frente\n",
    "    stopfound=False\n",
    "    for stp in stopnmb:\n",
    "        stoppos=np.where(np.array(seq)==stp)[0]\n",
    "        if len(stoppos)>0:\n",
    "            if len(stoppos)==1 and (stoppos[0]==nprm-1): # last stop found\n",
    "                laststop = 1\n",
    "            else:\n",
    "                print(\" stop codon \",stp,\" found in frame in sequence \",alignment[ctr].id,\" at positions \",stoppos[0])\n",
    "                stopfound=True\n",
    "                break\n",
    "\n",
    "    if not stopfound: # somente append as que nao tem stop codons in frame\n",
    "        filtered_SeqID.append(idlst[ctr]) # new 05.10.2023\n",
    "        filtered_Label.append(labels[ctr])\n",
    "        filtered_Seqs.append(sequences[ctr])\n",
    "        if laststop>0:\n",
    "            filtered_codonSeqs.append(seq[:-laststop]) #excluindo o ultimo stop se existir\n",
    "        else:\n",
    "            filtered_codonSeqs.append(seq)\n",
    "    ctr+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9a56b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sequences before filtering:  500  after:  500\n",
      "length in codons of sequences before filtering:  1274  after:  1273\n"
     ]
    }
   ],
   "source": [
    "# check stop codon filtering\n",
    "\n",
    "Nseq = len(filtered_codonSeqs)\n",
    "print(\"number of sequences before filtering: \",len(codonSequences),\" after: \",Nseq)\n",
    "print(\"length in codons of sequences before filtering: \",len(codonSequences[0]),\" after: \",\\\n",
    "      len(filtered_codonSeqs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f1ad7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 1273)\n"
     ]
    }
   ],
   "source": [
    "# transformando o dataset em matriz de codons\n",
    "\n",
    "cdnseq=np.array(filtered_codonSeqs)\n",
    "print(cdnseq.shape)\n",
    "\n",
    "# contando número de codons distintos por coluna (sítios)\n",
    "cdns=[]\n",
    "V=[]\n",
    "\n",
    "for pos in range(cdnseq.shape[1]):\n",
    "    cdnlst=list(set(list(cdnseq[:,pos])))\n",
    "    V.append(len(cdnlst))\n",
    "    cdns.append(cdnlst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "febb9b9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID size  572  % of informative codons: 44.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12999/3473680594.py:12: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Var_Sites.append(int(site))\n"
     ]
    }
   ],
   "source": [
    "V=np.array(V)\n",
    "\n",
    "# ordeno as colunas de maior a menor variabilidade\n",
    "# Idx=np.argsort(-V)\n",
    "# print(\"Idx\",Idx)\n",
    "# print(\"V[Idx]\",V[Idx])\n",
    "\n",
    "# seleciono as colunas com variabilidade > 1\n",
    "var_sites=np.argwhere(V>1)\n",
    "Var_Sites=[]\n",
    "for site in var_sites:\n",
    "    Var_Sites.append(int(site))\n",
    "\n",
    "var=V[var_sites]\n",
    "\n",
    "# print(\"var\",var)\n",
    "\n",
    "IDsz = len(var_sites)\n",
    "print(\"ID size \",IDsz,f\" % of informative codons: {100*IDsz/nprm:.2f}\")\n",
    "\n",
    "\n",
    "# gero os IDs com os codons de cada seq (lista de listas)\n",
    "\n",
    "ID = cdnseq[:,var_sites].tolist() # vetor coluna de IDs como vetores linha\n",
    "\n",
    "strID = [] # lista de IDs como strings\n",
    "for lst in ID:\n",
    "    strID.append(str(lst).replace('[','').replace(']',''))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1e2f58",
   "metadata": {},
   "source": [
    "# CBUC\n",
    "## gero a lista de string IDs das classes primárias - unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d2f02da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check primary class 0 and its accession, string ID and char vector ID and FASTA\n",
      "NUMBER OF THE PRIMARY CLASS:  0  ACCESSION:  spike_OM056186\n",
      "STRING ID: \n",
      " 47, 31, 60, 40, 30, 59, 7, 31, 47, 8, 10, 11, 60, 42, 43, 24, 50, 11, 3, 62, 8, 31, 50, 58, 35, 33, 18, 61, 62, 60, 43, 58, 3, 31, 11, 53, 35, 31, 64, 64, 64, 10, 23, 1, 5, 63, 19, 2, 43, 30, 44, 40, 63, 3, 19, 23, 63, 27, 58, 15, 17, 1, 59, 2, 12, 53, 15, 63, 23, 11, 11, 60, 19, 57, 1, 10, 33, 58, 44, 47, 15, 31, 3, 2, 27, 11, 3, 31, 31, 15, 63, 55, 3, 19, 40, 63, 61, 64, 64, 64, 64, 34, 0, 7, 53, 17, 62, 4, 31, 51, 59, 7, 25, 3, 3, 54, 11, 16, 51, 30, 59, 33, 43, 63, 47, 18, 20, 0, 33, 23, 3, 62, 47, 5, 16, 63, 29, 63, 1, 15, 19, 23, 12, 1, 43, 15, 15, 12, 19, 46, 43, 57, 27, 40, 61, 28, 2, 14, 11, 5, 11, 60, 47, 27, 60, 35, 19, 59, 59, 56, 23, 53, 8, 27, 23, 27, 24, 27, 51, 51, 29, 23, 51, 47, 32, 43, 5, 11, 63, 44, 60, 0, 51, 3, 16, 3, 20, 10, 15, 8, 19, 27, 28, 18, 55, 24, 43, 46, 56, 16, 8, 1, 55, 9, 61, 0, 58, 62, 11, 28, 16, 0, 20, 14, 51, 32, 11, 59, 2, 63, 4, 30, 32, 40, 8, 16, 59, 15, 31, 4, 54, 63, 19, 63, 2, 26, 4, 24, 31, 51, 2, 14, 59, 58, 56, 63, 58, 11, 47, 11, 3, 30, 51, 24, 4, 14, 27, 21, 32, 1, 15, 27, 19, 8, 54, 31, 12, 27, 53, 3, 59, 2, 3, 47, 19, 59, 1, 23, 51, 50, 37, 51, 4, 61, 63, 5, 1, 3, 46, 0, 43, 63, 17, 4, 19, 15, 51, 33, 26, 23, 6, 0, 43, 55, 3, 23, 31, 16, 55, 50, 43, 32, 56, 51, 23, 62, 32, 3, 23, 31, 23, 50, 32, 40, 50, 47, 44, 35, 24, 11, 31, 55, 20, 43, 0, 59, 11, 3, 31, 3, 23, 60, 0, 8, 17, 1, 27, 11, 11, 27, 39, 40, 17, 8, 20, 2, 33, 31, 31, 33, 23, 8, 16, 27, 11, 43, 11, 53, 39, 8, 59, 21, 27, 16, 51, 30, 2, 2, 56, 51, 18, 12, 42, 15, 27, 33, 11, 33, 1, 59, 35, 37, 37, 24, 39, 7, 28, 27, 7, 32, 58, 14, 15, 26, 50, 11, 13, 56, 24, 56, 3, 2, 59, 15, 12, 42, 8, 3, 11, 15, 7, 31, 10, 15, 44, 40, 29, 59, 13, 10, 8, 56, 28, 19, 55, 19, 3, 55, 8, 32, 60, 0, 39, 27, 20, 27, 10, 32, 24, 0, 8, 51, 40, 40, 44, 47, 2, 29, 24, 19, 14, 19, 27, 46, 0, 22, 11, 40, 27, 25, 23, 27, 27, 15, 26, 2, 3, 7, 27, 15, 18, 47, 59, 20, 19, 29, 35, 2, 9, 0, 7, 63, 56, 61, 22, 33, 11, 14, 27, 11, 22, 56, 43, 30, 11, 30, 43, 1, 11, 15, 20, 43, 34, 28, 16, 32, 14, 11, 8, 18, 2, 8, 23, 19, 2, 19, 56, 17, 19, 51, 1, 35, 56, 40, 19, 23, 22, 3, 31, 2, 15, 1, 14, 19, 46, 32, 16, 47, 17, 23, 26, 28, 13, 54, 13, 10, 55, 55, 55, 40, 29, 20\n",
      "VECTOR ID: \n",
      " ['47' '31' '60' '40' '30' '59' '7' '31' '47' '8' '10' '11' '60' '42' '43'\n",
      " '24' '50' '11' '3' '62' '8' '31' '50' '58' '35' '33' '18' '61' '62' '60'\n",
      " '43' '58' '3' '31' '11' '53' '35' '31' '64' '64' '64' '10' '23' '1' '5'\n",
      " '63' '19' '2' '43' '30' '44' '40' '63' '3' '19' '23' '63' '27' '58' '15'\n",
      " '17' '1' '59' '2' '12' '53' '15' '63' '23' '11' '11' '60' '19' '57' '1'\n",
      " '10' '33' '58' '44' '47' '15' '31' '3' '2' '27' '11' '3' '31' '31' '15'\n",
      " '63' '55' '3' '19' '40' '63' '61' '64' '64' '64' '64' '34' '0' '7' '53'\n",
      " '17' '62' '4' '31' '51' '59' '7' '25' '3' '3' '54' '11' '16' '51' '30'\n",
      " '59' '33' '43' '63' '47' '18' '20' '0' '33' '23' '3' '62' '47' '5' '16'\n",
      " '63' '29' '63' '1' '15' '19' '23' '12' '1' '43' '15' '15' '12' '19' '46'\n",
      " '43' '57' '27' '40' '61' '28' '2' '14' '11' '5' '11' '60' '47' '27' '60'\n",
      " '35' '19' '59' '59' '56' '23' '53' '8' '27' '23' '27' '24' '27' '51' '51'\n",
      " '29' '23' '51' '47' '32' '43' '5' '11' '63' '44' '60' '0' '51' '3' '16'\n",
      " '3' '20' '10' '15' '8' '19' '27' '28' '18' '55' '24' '43' '46' '56' '16'\n",
      " '8' '1' '55' '9' '61' '0' '58' '62' '11' '28' '16' '0' '20' '14' '51'\n",
      " '32' '11' '59' '2' '63' '4' '30' '32' '40' '8' '16' '59' '15' '31' '4'\n",
      " '54' '63' '19' '63' '2' '26' '4' '24' '31' '51' '2' '14' '59' '58' '56'\n",
      " '63' '58' '11' '47' '11' '3' '30' '51' '24' '4' '14' '27' '21' '32' '1'\n",
      " '15' '27' '19' '8' '54' '31' '12' '27' '53' '3' '59' '2' '3' '47' '19'\n",
      " '59' '1' '23' '51' '50' '37' '51' '4' '61' '63' '5' '1' '3' '46' '0' '43'\n",
      " '63' '17' '4' '19' '15' '51' '33' '26' '23' '6' '0' '43' '55' '3' '23'\n",
      " '31' '16' '55' '50' '43' '32' '56' '51' '23' '62' '32' '3' '23' '31' '23'\n",
      " '50' '32' '40' '50' '47' '44' '35' '24' '11' '31' '55' '20' '43' '0' '59'\n",
      " '11' '3' '31' '3' '23' '60' '0' '8' '17' '1' '27' '11' '11' '27' '39'\n",
      " '40' '17' '8' '20' '2' '33' '31' '31' '33' '23' '8' '16' '27' '11' '43'\n",
      " '11' '53' '39' '8' '59' '21' '27' '16' '51' '30' '2' '2' '56' '51' '18'\n",
      " '12' '42' '15' '27' '33' '11' '33' '1' '59' '35' '37' '37' '24' '39' '7'\n",
      " '28' '27' '7' '32' '58' '14' '15' '26' '50' '11' '13' '56' '24' '56' '3'\n",
      " '2' '59' '15' '12' '42' '8' '3' '11' '15' '7' '31' '10' '15' '44' '40'\n",
      " '29' '59' '13' '10' '8' '56' '28' '19' '55' '19' '3' '55' '8' '32' '60'\n",
      " '0' '39' '27' '20' '27' '10' '32' '24' '0' '8' '51' '40' '40' '44' '47'\n",
      " '2' '29' '24' '19' '14' '19' '27' '46' '0' '22' '11' '40' '27' '25' '23'\n",
      " '27' '27' '15' '26' '2' '3' '7' '27' '15' '18' '47' '59' '20' '19' '29'\n",
      " '35' '2' '9' '0' '7' '63' '56' '61' '22' '33' '11' '14' '27' '11' '22'\n",
      " '56' '43' '30' '11' '30' '43' '1' '11' '15' '20' '43' '34' '28' '16' '32'\n",
      " '14' '11' '8' '18' '2' '8' '23' '19' '2' '19' '56' '17' '19' '51' '1'\n",
      " '35' '56' '40' '19' '23' '22' '3' '31' '2' '15' '1' '14' '19' '46' '32'\n",
      " '16' '47' '17' '23' '26' '28' '13' '54' '13' '10' '55' '55' '55' '40'\n",
      " '29' '20']\n",
      "NUCLEOTIDE SEQUENCE: \n",
      " ATGTTTGTTTTTCTTGTTTTATTGCCACTAGTCTCTAGTCAGTGTGTTAATCTTACAACCAGAACTCAATTACCCCCTGCATACACTAATTCTTTCACACGTGGTGTTTATTACCCTGACAAAGTTTTCAGATCCTCAGTTTTACATTCAACTCAGGACTTGTTCTTACCTTTCTTTTCCAATGTTACTTGGTTCCATGTTA------TCTCTGGGACCAATGGTACTAAGAGGTTTGATAACCCTGTCCTACCATTTAATGATGGTGTTTATTTTGCTTCCATTGAGAAGTCTAACATAATAAGAGGCTGGATTTTTGGTACTACTTTAGATTCGAAGACCCAGTCCCTACTTATTGTTAATAACGCTACTAATGTTGTTATTAAAGTCTGTGAATTTCAATTTTGTAATGATCCATTTTTGG---------ACCACAAAAACAACAAAAGTTGGATGGAAAGTGAGTTCAGAGTTTATTCTAGTGCGAATAATTGCACTTTTGAATATGTCTCTCAGCCTTTTCTTATGGACCTTGAAGGAAAACAGGGTAATTTCAAAAATCTTAGGGAATTTGTGTTTAAGAATATTGATGGTTATTTTAAAATATATTCTAAGCACACGCCTATTATTATAGTGCGTGATCTCCCTCAGGGTTTTTCGGCTTTAGAACCATTGGTAGATTTGCCAATAGGTATTAACATCACTAGGTTTCAAACTTTACTTGCTTTACATAGAAGTTATTTGACTCCTGGTGATTCTTCTTCAGGTTGGACAGCTGGTGCTGCAGCTTATTATGTGGGTTATCTTCAACCTAGGACTTTTCTATTAAAATATAATGAAAATGGAACCATTACAGATGCTGTAGACTGTGCACTTGACCCTCTCTCAGAAACAAAGTGTACGTTGAAATCCTTCACTGTAGAAAAAGGAATCTATCAAACTTCTAACTTTAGAGTCCAACCAACAGAATCTATTGTTAGATTTCCTAATATTACAAACTTGTGCCCTTTTGATGAAGTTTTTAACGCCACCAGATTTGCATCTGTTTATGCTTGGAACAGGAAGAGAATCAGCAACTGTGTTGCTGATTATTCTGTCCTATATAATTCCGCATCATTTTCCACTTTTAAGTGTTATGGAGTGTCTCTTACTAAATTAAATGATCTCTGCTTTACTAATGTCTATGCAGATTCATTTGTAATTAGAGGTGATGAAGTCAGACAAATCGCTCCAGGGCAAACTGGAAAGATTGCTGATTATAATTATAAATTACCAGATGATTTTACAGGCTGCGTTATAGCTTGGAATTCTAACAATCTTGATTCTAAGGTTGGTGGTAATTATAATTACCGGTATAGATTGTTTAGGAAGTCTAATCTCAAACCTTTTGAGAGAGATATTTCAACTGAAATCTATCAGGCCGGTAGCAAACCTTGTAATGGTGTTGAAGGTTTTAATTGTTACTTTCCTTTACAATCATATGGTTTCCAACCCACTAATGGTGTTGGTTACCAACCATACAGAGTAGTAGTACTTTCTTTTGAACTTCTACATGCACCAGCAACTGTTTGTGGACCTAAAAAGTCTACTAATTTGGTTAAAAACAAATGTGTCAATTTCAACTTCAATGGTTTAAAAGGCACAGGTGTTCTTACTGAGTCTAACAAAAAGTTTCTGCCTTTCCAACAATTTGGCAGAGACATTGCTGACACTACTGATGCTGTCCGTGATCCACAGACACTTGAGATTCTTGACATTACACCATGTTCTTTTGGTGGTGTCAGTGTTATAACACCAGGAACAAATACTTCTAACCAGGTTGCTGTTCTTTATCAGGGTGTTAACTGCACAGAAGTCCCTGTTGCTATTCATGCAGATCAACTTACTCCTACTTGGCGTGTTTATTCTACAGGTTCTAATGTTTTTCAAACACGTGCAGGCTGTTTAATAGGGGCTGAATATGTCAACAACTCATATGAGTGTGACATACCCATTGGTGCAGGTATATGCGCTAGTTATCAGACTCAGACTAAGTCTCATCGGCGGGCACGTAGTGTAGCTAGTCAATCCATCATTGCCTACACTATGTCACTTGGTGCAGAAAATTCAGTTGCTTACTCTAATAACTCTATTGCCATACCCACAAATTTTACTATTAGTGTTACCACAGAAATTCTACCAGTGTCTATGACCAAGACATCAGTAGATTGTACAATGTACATTTGTGGTGATTCAACTGAATGCAGCAATCTTTTGTTGCAATATGGCAGTTTTTGTACACAATTAAAACGTGCTTTAACTGGAATAGCTGTTGAACAAGACAAAAACACCCAAGAAGTTTTTGCACAAGTCAAACAAATTTACAAAACACCACCAATTAAATATTTTGGTGGTTTTAATTTTTCACAAATATTACCAGATCCATCAAAACCAAGCAAGAGGTCATTTATTGAAGATCTACTTTTCAACAAAGTGACACTTGCAGATGCTGGCTTCATCAAACAATATGGTGATTGCCTTGGTGATATTGCTGCTAGAGACCTCATTTGTGCACAAAAGTTTAAAGGCCTTACTGTTTTGCCACCTTTGCTCACAGATGAAATGATTGCTCAATACACTTCTGCACTGTTAGCGGGTACAATCACTTCTGGTTGGACCTTTGGTGCAGGTGCTGCATTACAAATACCATTTGCTATGCAAATGGCTTATAGGTTTAATGGTATTGGAGTTACACAGAATGTTCTCTATGAGAACCAAAAATTGATTGCCAACCAATTTAATAGTGCTATTGGCAAAATTCAAGACTCACTTTCTTCCACAGCAAGTGCACTTGGAAAACTTCAAGATGTGGTCAACCATAATGCACAAGCTTTAAACACGCTTGTTAAACAACTTAGCTCCAAATTTGGTGCAATTTCAAGTGTTTTAAATGATATCTTTTCACGTCTTGACAAAGTTGAGGCTGAAGTGCAAATTGATAGGTTGATCACAGGCAGACTTCAAAGTTTGCAGACATATGTGACTCAACAATTAATTAGAGCTGCAGAAATCAGAGCTTCTGCTAATCTTGCTGCTACTAAAATGTCAGAGTGTGTACTTGGACAATCAAAAAGAGTTGATTTTTGTGGAAAGGGCTATCATCTTATGTCCTTCCCTCAGTCAGCACCTCATGGTGTAGTCTTCTTGCATGTGACTTATGTCCCTGCACAAGAAAAGAACTTCACAACTGCTCCTGCCATTTGTCATGATGGAAAAGCACACTTTCCTCGTGAAGGTGTCTTTGTTTCAAATGGCACACACTGGTTTGTAACACAAAGGAATTTTTATGAACCACAAATCATTACTACAGACAACACATTTGTGTCTGGTAACTGTGATGTTGTAATAGGAATTGTCAACAACACAGTTTATGATCCTTTGCAACCTGAATTAGATTCATTCAAGGAGGAGTTAGATAAATATTTTAAGAATCATACATCACCAGATGTTGATTTAGGTGACATCTCTGGCATTAATGCTTCAGTTGTAAACATTCAAAAAGAAATTGACCGCCTCAATGAGGTTGCCAAGAATTTAAATGAATCTCTCATCGATCTCCAAGAACTTGGAAAGTATGAGCAGTATATAAAATGGCCATGGTACATTTGGCTAGGTTTTATAGCTGGCTTGATTGCCATAGTAATGGTGACAATTATGCTTTGCTGTATGACCAGTTGCTGTAGTTGTCTCAAGGGCTGTTGTTCTTGTGGATCCTGCTGCAAATTTGATGAAGACGACTCTGAGCCAGTGCTCAAAGGAGTCAAATTACATTACACATAA\n",
      "there were found  500  primary classes in the dataset with  500  sequences in  0.024855375289916992  segundos\n",
      "different sequences: 100.00 %\n"
     ]
    }
   ],
   "source": [
    "# gero a lista de string IDs das classes primárias - unique\n",
    "start = time.time()\n",
    "p = [] # vetor com string das IDs das classes primárias\n",
    "pvec = []\n",
    "pClass = []\n",
    "pAcc = []\n",
    "pSeq = []\n",
    "P=0\n",
    "ctr=0\n",
    "for Id in strID:\n",
    "    if Id not in p: # filtro sequencias iguais\n",
    "        p.append(Id)\n",
    "        pvec.append(np.array(Id.split(\", \")))\n",
    "        pClass.append(P) # numero a classe primaria\n",
    "        pAcc.append(filtered_SeqID[ctr]) # capturo o accession\n",
    "        pSeq.append(filtered_Seqs[ctr]) # capturo o accession\n",
    "        P+=1\n",
    "    ctr+=1\n",
    "\n",
    "print(\"check primary class 0 and its accession, string ID and char vector ID and FASTA\")\n",
    "print(\"NUMBER OF THE PRIMARY CLASS: \",pClass[0],\" ACCESSION: \",pAcc[0])\n",
    "print(\"STRING ID: \\n\",p[0])\n",
    "print(\"VECTOR ID: \\n\",pvec[0])\n",
    "print(\"NUCLEOTIDE SEQUENCE: \\n\",pSeq[0])\n",
    "\n",
    "# numero de classes primarias\n",
    "print(\"there were found \",P,\" primary classes in the dataset with \",Nseqs,\" sequences in \",\\\n",
    "      time.time()-start,\" segundos\")\n",
    "print(f\"different sequences: {P/Nseq*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79d8e901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 = 500\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "print(len(pvec),\"=\",P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2511fe",
   "metadata": {},
   "source": [
    "# criando dataframe com o Gene Variant File  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43003b90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>accession</th>\n",
       "      <th>fasta_aln</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>spike_OM056186</td>\n",
       "      <td>(A, T, G, T, T, T, G, T, T, T, T, T, C, T, T, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>spike_OM058922</td>\n",
       "      <td>(A, T, G, T, T, T, G, T, T, T, T, T, C, T, T, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>spike_OM059019</td>\n",
       "      <td>(A, T, G, T, T, T, G, T, T, T, T, T, C, T, T, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>spike_OM059033</td>\n",
       "      <td>(A, T, G, T, T, T, G, T, T, T, T, T, C, T, T, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>spike_OM059246</td>\n",
       "      <td>(A, T, G, T, T, T, G, T, T, T, T, T, C, T, T, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>495</td>\n",
       "      <td>spike_MZ394617</td>\n",
       "      <td>(A, T, G, T, T, T, G, T, T, T, T, T, C, T, T, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>496</td>\n",
       "      <td>spike_MZ342413</td>\n",
       "      <td>(A, T, G, T, T, T, G, T, T, T, T, T, C, T, T, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>497</td>\n",
       "      <td>spike_MZ450533</td>\n",
       "      <td>(A, T, G, T, T, T, G, T, T, T, T, T, C, T, T, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>498</td>\n",
       "      <td>spike_MZ378667</td>\n",
       "      <td>(A, T, G, T, T, T, G, T, T, T, T, T, C, T, T, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>499</td>\n",
       "      <td>spike_OU070204</td>\n",
       "      <td>(A, T, G, T, T, T, G, T, T, T, T, T, C, T, T, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class       accession                                          fasta_aln\n",
       "0        0  spike_OM056186  (A, T, G, T, T, T, G, T, T, T, T, T, C, T, T, ...\n",
       "1        1  spike_OM058922  (A, T, G, T, T, T, G, T, T, T, T, T, C, T, T, ...\n",
       "2        2  spike_OM059019  (A, T, G, T, T, T, G, T, T, T, T, T, C, T, T, ...\n",
       "3        3  spike_OM059033  (A, T, G, T, T, T, G, T, T, T, T, T, C, T, T, ...\n",
       "4        4  spike_OM059246  (A, T, G, T, T, T, G, T, T, T, T, T, C, T, T, ...\n",
       "..     ...             ...                                                ...\n",
       "495    495  spike_MZ394617  (A, T, G, T, T, T, G, T, T, T, T, T, C, T, T, ...\n",
       "496    496  spike_MZ342413  (A, T, G, T, T, T, G, T, T, T, T, T, C, T, T, ...\n",
       "497    497  spike_MZ450533  (A, T, G, T, T, T, G, T, T, T, T, T, C, T, T, ...\n",
       "498    498  spike_MZ378667  (A, T, G, T, T, T, G, T, T, T, T, T, C, T, T, ...\n",
       "499    499  spike_OU070204  (A, T, G, T, T, T, G, T, T, T, T, T, C, T, T, ...\n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# criando dataframe com o Gene Variant File\n",
    "\n",
    "UniqueSeqs   = pSeq\n",
    "UniqueSeqIDs = pAcc\n",
    "UniqueClass  = pClass\n",
    "\n",
    "gvf = pd.DataFrame({'class':pClass,'accession':pAcc,'fasta_aln':pd.Series(UniqueSeqs)}) #,index=0)\n",
    "\n",
    "gvf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb80379d",
   "metadata": {},
   "source": [
    "## controle de qualidade do dataset de treinamento\n",
    "### quantas sequencias tem de cada classe primária?\n",
    "### >>>>> deve ser 1 só"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15f298fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO REDUNDANT SEQUENCES HAVE BEEN FOUND IN THE TRAINING DATASET - PASSED ...\n"
     ]
    }
   ],
   "source": [
    "# histograma de frequencia das classes primárias - depende do ordenamento\n",
    "from collections import Counter\n",
    "\n",
    "counts = Counter(pClass)\n",
    "# print(counts)\n",
    "numero_mais_comum, frequencia_mais_comum = counts.most_common(1)[0]\n",
    "\n",
    "if frequencia_mais_comum > 1:\n",
    "    print(\"WARNING: THERE ARE REDUNDANT SEQUENCES IN THE TRAINING DATASET \")\n",
    "    # Exiba o número mais comum e sua frequência\n",
    "    print(f\"The most common class is {numero_mais_comum} with {frequencia_mais_comum} copies\")\n",
    "\n",
    "    plt.figure(figsize=(14,2))\n",
    "    plt.hist(pClass,P)\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"primary class\")\n",
    "    plt.ylabel(\"# of seqs by primary class\")\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"NO REDUNDANT SEQUENCES HAVE BEEN FOUND IN THE TRAINING DATASET - PASSED ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd10b2ea",
   "metadata": {},
   "source": [
    "## DEFINING THE CLASS OF THE THE AGUA MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d645b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, name = \"\", rep=2, nmbofclasses=0, nmbofclusters=0):\n",
    "\n",
    "        self.name = name\n",
    "        self.ListOfVarSites =[] #CBUC\n",
    "        self.NmbOfClasses = nmbofclasses # CBUC\n",
    "        self.CodeOfClass = [] # CBUC\n",
    "        self.GroundTruth = [] # ANNOTATION\n",
    "        self.NmbOfClusters =  nmbofclusters # CLOPE\n",
    "        self.TheClusterOfClass = {} # CLOPE\n",
    "        self.TheSpeciesOfCluster = {} # CLOPE + ANNOTATION\n",
    "        self.repulsion = rep # CLOPE\n",
    "        self.MinClusterResolution = 0 #  MODEL\n",
    "        self.Resolution = 0 # MODEL\n",
    "        self.DistributionMatrix =[] # MODEL\n",
    "        self.Accession = [] # DATA - GVF\n",
    "        self.CLOPE_repulsion = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "746a4ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\"SARS2-spike2\",nmbofclasses=P)\n",
    "model.ListOfVarSites = Var_Sites #CBUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8926b3e",
   "metadata": {},
   "source": [
    "# FASE 2 - ANÁLISE QUANTITATIVA\n",
    "\n",
    "## 1. AGRUPAMENTO \n",
    "## 2. ANOTAÇÃO DAS CLASSES PRIMÁRIAS\n",
    "## 3. CÁLCULO DA RESOLUÇÃO DO CLASSIFICADOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bc1553",
   "metadata": {},
   "source": [
    "### CLUSTERIZADOR NÃO SUPERVISIONADO - CLOPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7eb28de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes e funções da clusterização com CLOPE\n",
    "\n",
    "class Cluster:\n",
    "\n",
    "    def __init__(self, history_count):\n",
    "       # Histórico do número de transações em clusters\n",
    "        self.history_count_transact = [0] * history_count\n",
    "        # Área do histograma\n",
    "        self.area = 0.0\n",
    "        # A altura do histograma (no sentido de H=S/W). Esse valor não é calculado explicitamente em nenhum lugar.\n",
    "         # Armazenado na classe para completar a descrição da classe e nada mais.\n",
    "        self.height = 0.0\n",
    "        # Largura do histograma (em termos de número de elementos)\n",
    "        self.width = 0.0\n",
    "        # Gradiente (no sentido de G=H/W). Esse valor não é calculado explicitamente em nenhum lugar.\n",
    "        # Armazenado na classe para completar a descrição da classe e nada mais.\n",
    "        self.gradient = 0.0\n",
    "        # Número de transações\n",
    "        self.count_transactions = 0\n",
    "        # Histograma\n",
    "        self.histogram = {}\n",
    "\n",
    "    '''\n",
    "    Adicione uma transação ao cluster. Iterar sobre todos os elementos do histograma, completar o histograma\n",
    "     parâmetros de entrada:\n",
    "     transação -- fatia com objetos (transação)\n",
    "    '''\n",
    "    def add_transaction(self, transaction):\n",
    "        # Iterar por todos os elementos do histograma um por um e adicionar à coluna correspondente do histograma.\n",
    "        # Se não há elemento em questão, então adicione uma nova coluna ao histograma\n",
    "        for item in transaction:\n",
    "            if not (item in self.histogram):\n",
    "                self.histogram[item] = 1 # adiciona novo elelento ao histograma\n",
    "            else:\n",
    "                self.histogram[item] += 1 # incrementa o numero do elemento existente no histograma\n",
    "        # Calculamos a área total do histograma no sentido de CLOPE (número de transações)\n",
    "        self.area += float(len(transaction))\n",
    "        # Calcular a largura do histograma (o número de objetos diferentes)\n",
    "        self.width = float(len(self.histogram))\n",
    "        # incrementa o número de transações no cluster\n",
    "        self.count_transactions += 1\n",
    "\n",
    "    '''\n",
    "    Excluir transação do cluster. Passamos por todos os elementos do histograma, removemos todos os elementos da transação de\n",
    "     histogramas\n",
    "\n",
    "     parâmetros de entrada:\n",
    "     transação -- fatia com objetos (transação)\n",
    "     valores retornados:\n",
    "     valor gradiente G (transação) # sem sentido\n",
    "\n",
    "     Dentro da classe, não há rastreamento de quais transações são adicionadas, quais são excluídas, portanto, se em\n",
    "     o processo de modificação excluirá uma transação que não foi adicionada ao cluster correspondente, o algoritmo\n",
    "     vai dar resultado errado\n",
    "     '''\n",
    "\n",
    "    def remove_transaction(self, transaction):\n",
    "            for item in transaction:\n",
    "                if self.histogram[item] > 0: # new\n",
    "                    self.histogram[item] -=1 # new\n",
    "                if self.histogram[item] == 0:\n",
    "                    del self.histogram[item]\n",
    "            self.area -= float(len(transaction))\n",
    "            self.width = float(len(self.histogram))\n",
    "            self.count_transactions -= 1\n",
    "            return # self.gradient # rare\n",
    "\n",
    "class CLOPE:\n",
    "\n",
    "    def __init__(self, print_step=1000, random_seed=None):\n",
    "\n",
    "        if random_seed is not None:\n",
    "            self.random_seed = random_seed\n",
    "        else:\n",
    "            self.random_seed = random.randint(0, 65536 + 1)\n",
    "\n",
    "        # Lista de clusters\n",
    "        self.clusters = {}  # CCluster\n",
    "        # Números de clusters de ruído\n",
    "        # Este objeto é necessário para não levar em consideração aqueles objetos que foram classificados como ruído\n",
    "        self.noise_clusters = {}\n",
    "        # Número de transações adicionadas\n",
    "        self.count_transactions = 0\n",
    "        # Número da iteração\n",
    "        self.iteration = 0\n",
    "        # inicializa dicionário: key = número ORIGINAL (no dict Data) da transação/número do cluster atribuido\n",
    "        self.transaction = {}\n",
    "        # Número máximo do cluster\n",
    "        self.max_cluster_number = 0\n",
    "        self.print_step = print_step\n",
    "\n",
    "    '''\n",
    "    A mudança de meta que a função objetivo receberá ao adicionar uma transação ao cluster clusterNumber é calculada.\n",
    "     O cluster que entrega o valor máximo da função será o cluster desejado (ao qual se deve adicionar a transação)\n",
    "     parâmetros de entrada:\n",
    "     transação -- transação (lista de objetos)\n",
    "     clusterNumber -- número do cluster cujo incremento está sendo calculado\n",
    "     r -- repulsão no sentido de CLOPE\n",
    "     valor retornado:\n",
    "     Retorna o valor de alteração da função objetivo quando uma transação é adicionada ao cluster clusterNumber\n",
    "    '''\n",
    "    def get_delta(self, transaction, cluster_number, r):\n",
    "\n",
    "        old_area = self.clusters[cluster_number].area\n",
    "        old_width = self.clusters[cluster_number].width\n",
    "        nmb_trans = self.clusters[cluster_number].count_transactions\n",
    "\n",
    "        area = old_area + len(transaction) # calcula a área adicionando os objetos da transação\n",
    "                                                                    # em análise\n",
    "        # cálculo da largura do cluster\n",
    "        width = old_width\n",
    "        for item in transaction:\n",
    "            if not (item in self.clusters[cluster_number].histogram): # histogram tem a lista de objetos no cluster\n",
    "                width += 1\n",
    "\n",
    "        # calcula delta (variação da função objetivo com a adição da transação ao cluster cluster_number)\n",
    "        new_delta_value = 0\n",
    "        if width != 0:\n",
    "            new_delta_value = area * (nmb_trans + 1) / (width ** r)\n",
    "\n",
    "        # calcula o valor de delta sem a transação\n",
    "        if old_width != 0:\n",
    "            old_delta_value = old_area * nmb_trans / (old_width ** r)\n",
    "        else:\n",
    "            old_delta_value = 0\n",
    "\n",
    "        return new_delta_value - old_delta_value\n",
    "\n",
    "    '''\n",
    "    Função de remoção de ruído. Todos os clusters maiores que o limite permanecem ativos.\n",
    "     parâmetros de entrada:\n",
    "     limite -- nível de ruído do cluster\n",
    "    '''\n",
    "    def noise_reduction(self, limit):\n",
    "        # Remova todos os clusters vazios e barulhentos\n",
    "        new_clusters = {}\n",
    "        for item in self.clusters:\n",
    "            if self.clusters[item].count_transactions > limit:# se o cluster é maior que o limite permanece na lista de clusters ativos\n",
    "                new_clusters[item] = self.clusters[item]\n",
    "            else: # clusters pequenos são considerados ruidosos e saem da lista de clusters ativo -\n",
    "                  # passam para alista de clusters com ruido\n",
    "                self.noise_clusters[item] = True\n",
    "        self.clusters = new_clusters\n",
    "\n",
    "    '''\n",
    "    Cálculo da função objetivo para todos os clusters já formados\n",
    "     Usado ao modificar clusters ou inicializá-los\n",
    "     parâmetros de entrada:\n",
    "     r -- número real denotando repulsão de cluster no sentido de CLOPE\n",
    "     valor retornado:\n",
    "     Retorna o valor da função objetivo\n",
    "    '''\n",
    "    def get_goal_function(self, r):\n",
    "        measure = 0.0\n",
    "        # Percorremos todos os clusters e para cada um calculamos seu peso.\n",
    "        # Todos os pesos são resumidos em uma métrica comum\n",
    "        for item in self.clusters:\n",
    "            if item.width == 0:\n",
    "                # print \"test\"\n",
    "                pass\n",
    "            else:\n",
    "                # # Tentando encontrar uma métrica diferente\n",
    "                # measure += (r ** 2 * math.log(item.Area ** (1 / r) / item.Width) ** 2) *\n",
    "                # item.CountTransactions / self.CountTransactions\n",
    "                measure += item.area / (item.width ** r) * item.count_transactions\n",
    "        return measure / self.count_transactions\n",
    "\n",
    "    '''\n",
    "     Adicionando uma nova transação\n",
    "     Estamos tentando redistribuir a transação com o número de id em outra classe para que a função de custo tenha\n",
    "     valor máximo\n",
    "     parâmetros de entrada:\n",
    "     transação -- transação (fatia com objetos)\n",
    "     id -- número da transação\n",
    "     repulsão -- número real, denotando repulsão de clusters no sentido de CLOPE\n",
    "     isSaveHistory -- sinalizador definido se for necessário registrar o histórico do número de transações\n",
    "     parâmetro retornado:\n",
    "     Retorna o número do cluster ao qual a transação atual foi adicionada\n",
    "    '''\n",
    "    def move_transaction(self, transaction, id, repulsion=2, max_count_clusters=None):\n",
    "\n",
    "        r = repulsion\n",
    "        max_value = None\n",
    "        max_value_index = None\n",
    "        self.count_transactions += 1\n",
    "\n",
    "        # Estamos procurando um cluster no qual o valor máximo da mudança na função objetivo seja alcançado\n",
    "        for cluster_number in self.clusters:\n",
    "            delta = self.get_delta(transaction, cluster_number, r)\n",
    "            if (delta > 0 or max_count_clusters is not None) and (max_value is None or delta > max_value):\n",
    "                max_value_index = cluster_number\n",
    "                max_value = delta\n",
    "\n",
    "        # Adicione uma transação a um novo cluster e veja o resultado - registre o cluster com maior valor\n",
    "        if max_count_clusters is None or len(self.clusters) < max_count_clusters:\n",
    "            self.clusters[self.max_cluster_number] = Cluster(self.count_transactions)\n",
    "            if max_value is None or self.get_delta(transaction, self.max_cluster_number, r) > max_value:\n",
    "                max_value_index = self.max_cluster_number\n",
    "                self.max_cluster_number += 1\n",
    "            else:\n",
    "                del self.clusters[self.max_cluster_number]\n",
    "\n",
    "        # Lembramos em qual cluster está a transação atual\n",
    "        self.transaction[id] = max_value_index\n",
    "\n",
    "        #Adicionando uma transação ao cluster necessário\n",
    "        self.clusters[max_value_index].add_transaction(transaction)\n",
    "\n",
    "        return max_value_index\n",
    "\n",
    "    '''\n",
    "    Cálculo de limiar de ruído adaptativo. O limite é limpo em relação à mediana dos tamanhos de cluster (incluindo\n",
    "     transações). Pegue 3/4 da mediana\n",
    "    '''\n",
    "    def get_noise_limit(self, percentile=0.75):\n",
    "        size_clusters = []\n",
    "        for item in self.clusters:\n",
    "            size_clusters.append(self.clusters[item].count_transactions)\n",
    "        sorted(size_clusters)\n",
    "        median_element = int(len(size_clusters) * percentile) + 1\n",
    "        if len(size_clusters) < 5:\n",
    "            limit = 10\n",
    "        else:\n",
    "            limit = size_clusters[median_element]\n",
    "        return limit\n",
    "\n",
    "    '''\n",
    "    Inicialização do cluster\n",
    "    parâmetros de entrada:\n",
    "    dados -- fatia com transações\n",
    "    isPrint -- se deve imprimir informações de progresso (0 -- não é necessário, se > 0 -- imprimir a cada isPrint time)\n",
    "    repulsão -- número real, denotando repulsão de clusters no sentido de CLOPE\n",
    "    isSaveHistory -- sinalizador definido se for necessário registrar o histórico do número de transações\n",
    "    isNoiseReduction -- redução de ruído (limiar corresponde ao número de elementos no cluster no qual ele é destruído).\n",
    "                        Se isNoiseReduction == -1, então o limite é selecionado de forma adaptativa (tudo que for maior que a mediana\n",
    "                        restos)\n",
    "    '''\n",
    "    def init_clusters(self, data, repulsion=2, is_noise_reduction=-1, noise_median_threshold=0.75,\n",
    "                      max_count_clusters=None):\n",
    "        index = 0\n",
    "        keys = sorted(data.keys())\n",
    "        np.random.seed(self.random_seed)\n",
    "        np.random.shuffle(keys)\n",
    "        for item in keys:\n",
    "            self.move_transaction(data[item], item, repulsion, max_count_clusters)\n",
    "            index += 1\n",
    "            if self.print_step > 0 and index % self.print_step == 0:\n",
    "                print(\"ITERAÇÃO: \", self.iteration, \". NÚMERO DA ETAPA\", index, \". NÚMERO DE CLUSTERS: \", len(self.clusters))\n",
    "                pass\n",
    "\n",
    "        # Obtendo o limite de ruído ideal\n",
    "        if is_noise_reduction < 0:\n",
    "            is_noise_reduction = self.get_noise_limit(noise_median_threshold)\n",
    "        # Remova todos os clusters de ruído\n",
    "        # (após a inicialização, não há classes vazias, portanto, o sinal é estritamente maior)\n",
    "        if is_noise_reduction > 0:\n",
    "            self.noise_reduction(is_noise_reduction)\n",
    "\n",
    "        self.iteration = 1\n",
    "\n",
    "    '''\n",
    "    Execução do algoritmo. Dando o próximo passo\n",
    "    parâmetros de entrada:\n",
    "    dados -- fatia com transações\n",
    "    isPrint -- se deve imprimir informações de progresso (0 -- não é necessário, se > 0 -- imprimir a cada isPrint time)\n",
    "    repulsão -- número real, denotando repulsão de clusters no sentido de CLOPE\n",
    "    isSaveHistory -- sinalizador definido se for necessário registrar o histórico do número de transações\n",
    "    isNoiseReduction -- redução de ruído (limiar corresponde ao número de elementos no cluster no qual ele é destruído).\n",
    "                        Se isNoiseReduction == -1, então o limite é selecionado de forma adaptativa (tudo que for maior que a mediana\n",
    "                        restos)\n",
    "    parâmetro retornado:\n",
    "    Retorna o número de operações para transferir uma transação de cluster para cluster\n",
    "    '''\n",
    "    def next_step(self, data, repulsion=2, is_noise_reduction=-1, noise_median_threshold=0.75, max_count_clusters=None):\n",
    "\n",
    "        # Remova todos os clusters vazios (ou ruído, se isNoiseReduction > 0)\n",
    "        if is_noise_reduction < 0:\n",
    "            is_noise_reduction = self.get_noise_limit(noise_median_threshold)\n",
    "        self.noise_reduction(is_noise_reduction)\n",
    "\n",
    "        index = 0\n",
    "        # O número de transações que foram transferidas\n",
    "        moves = 0\n",
    "        keys = sorted(data.keys())\n",
    "        np.random.seed(self.random_seed)\n",
    "        np.random.shuffle(keys)\n",
    "        for id in keys:\n",
    "            # Nós olhamos onde esta transação está agora\n",
    "            cluster_number = self.transaction[id]\n",
    "            transaction = data[id]\n",
    "            # Se a transação pertencer a um cluster de ruído, não tentaremos alterá-la\n",
    "#             if cluster_number in self.noise_clusters:\n",
    "#                 eps += 0\n",
    "#             else:\n",
    "            # Recuperar uma transação do cluster atual\n",
    "            self.clusters[cluster_number].remove_transaction(transaction)\n",
    "            # Consideramos a transação como recém-chegada e a adicionamos ao cluster onde o valor do destino\n",
    "            # máximiza a função custo\n",
    "            moves += int(self.move_transaction(transaction, id, repulsion, max_count_clusters) != cluster_number)\n",
    "\n",
    "            index += 1\n",
    "            if self.print_step is not None and self.print_step > 0 and index % self.print_step == 0:\n",
    "#                 print(\"Итерация: \", self.iteration, \". Номер шага\", index, \". Число кластеров: \", len(self.clusters))\n",
    "#                 print(\"ITERAÇÃO: \", self.iteration, \". NÚMERO DA ETAPA\", index, \". NÚMERO DE CLUSTERS: \", len(self.clusters))\n",
    "                pass\n",
    "\n",
    "        self.iteration += 1\n",
    "\n",
    "        self.noise_reduction(is_noise_reduction)\n",
    "\n",
    "        return moves\n",
    "\n",
    "    '''\n",
    "    Desenhamos um gráfico mostrando o número de transações em diferentes classes TO DO !!!!\n",
    "\n",
    "    '''\n",
    "    def print_history_count(self, repulsion, seed):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f329777",
   "metadata": {},
   "source": [
    "# acessando à anotação de cada classe primária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "604cbedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we read  500  annotated sequence accessions\n",
      " {'OM056186': 'B.1.1.529', 'OM058922': 'B.1.1.529', 'OM059019': 'B.1.1.529', 'OM059033': 'B.1.1.529', 'OM059246': 'B.1.1.529', 'OM065541': 'B.1.1.529', 'OM065547': 'B.1.1.529', 'OM065569': 'B.1.1.529', 'OM065570': 'B.1.1.529', 'OM065572': 'B.1.1.529', 'OM065582': 'B.1.1.529', 'OM065584': 'B.1.1.529', 'OM065586': 'B.1.1.529', 'OM065603': 'B.1.1.529', 'OM065661': 'B.1.1.529', 'OM065662': 'B.1.1.529', 'OM065665': 'B.1.1.529', 'OM065677': 'B.1.1.529', 'OM065684': 'B.1.1.529', 'OM065682': 'B.1.1.529', 'OM065685': 'B.1.1.529', 'OM065686': 'B.1.1.529', 'OM065688': 'B.1.1.529', 'OM065691': 'B.1.1.529', 'OM065696': 'B.1.1.529', 'OM065709': 'B.1.1.529', 'OM065712': 'B.1.1.529', 'OM065714': 'B.1.1.529', 'OM036675': 'B.1.1.529', 'OM036870': 'B.1.1.529', 'OM036966': 'B.1.1.529', 'OM038376': 'B.1.1.529', 'OM116753': 'B.1.1.529', 'OM133912': 'B.1.1.529', 'OM117210': 'B.1.1.529', 'OM117387': 'B.1.1.529', 'OM135128': 'B.1.1.529', 'OM135237': 'B.1.1.529', 'OM136750': 'B.1.1.529', 'OM136922': 'B.1.1.529', 'OM122019': 'B.1.1.529', 'OM125506': 'B.1.1.529', 'OM126138': 'B.1.1.529', 'OM127121': 'B.1.1.529', 'OL919777': 'B.1.1.529', 'OM103723': 'B.1.1.529', 'OM173411': 'B.1.1.529', 'OM173719': 'B.1.1.529', 'OM173785': 'B.1.1.529', 'OM174072': 'B.1.1.529', 'OM155060': 'B.1.1.529', 'OM155094': 'B.1.1.529', 'OM078510': 'B.1.1.529', 'OM078813': 'B.1.1.529', 'OM079097': 'B.1.1.529', 'OM156007': 'B.1.1.529', 'OM156059': 'B.1.1.529', 'OM156101': 'B.1.1.529', 'OM156219': 'B.1.1.529', 'OM156236': 'B.1.1.529', 'OM156250': 'B.1.1.529', 'OM188423': 'B.1.1.529', 'OM096580': 'B.1.1.529', 'OM096691': 'B.1.1.529', 'OM097184': 'B.1.1.529', 'OM098781': 'B.1.1.529', 'OM099739': 'B.1.1.529', 'OM099777': 'B.1.1.529', 'OM229610': 'B.1.1.529', 'OM229612': 'B.1.1.529', 'OM229648': 'B.1.1.529', 'OM229650': 'B.1.1.529', 'OM229697': 'B.1.1.529', 'OM229711': 'B.1.1.529', 'OM229763': 'B.1.1.529', 'OM229764': 'B.1.1.529', 'OM229804': 'B.1.1.529', 'OM229817': 'B.1.1.529', 'OM229854': 'B.1.1.529', 'OM229881': 'B.1.1.529', 'OM229893': 'B.1.1.529', 'OM229948': 'B.1.1.529', 'OM229954': 'B.1.1.529', 'OM229979': 'B.1.1.529', 'OM197075': 'B.1.1.529', 'OM197156': 'B.1.1.529', 'OM232855': 'B.1.1.529', 'OM197869': 'B.1.1.529', 'OM198452': 'B.1.1.529', 'OM236175': 'B.1.1.529', 'OM245422': 'B.1.1.529', 'OM245453': 'B.1.1.529', 'OM245454': 'B.1.1.529', 'OM245498': 'B.1.1.529', 'OM245504': 'B.1.1.529', 'OM245579': 'B.1.1.529', 'OM245615': 'B.1.1.529', 'OM245673': 'B.1.1.529', 'OM245706': 'B.1.1.529', 'OM245828': 'B.1.1.529', 'OV904299': 'B.1.1.7', 'OV904308': 'B.1.1.7', 'OV904311': 'B.1.1.7', 'OV904312': 'B.1.1.7', 'OV904313': 'B.1.1.7', 'OV904315': 'B.1.1.7', 'OV904316': 'B.1.1.7', 'OV904319': 'B.1.1.7', 'OV904321': 'B.1.1.7', 'OV904326': 'B.1.1.7', 'OV904327': 'B.1.1.7', 'OV904329': 'B.1.1.7', 'OV904332': 'B.1.1.7', 'OV904333': 'B.1.1.7', 'OV904338': 'B.1.1.7', 'OV904343': 'B.1.1.7', 'OV905059': 'B.1.1.7', 'OV905060': 'B.1.1.7', 'OV905061': 'B.1.1.7', 'OV905064': 'B.1.1.7', 'OV905065': 'B.1.1.7', 'OV905066': 'B.1.1.7', 'OV905069': 'B.1.1.7', 'OV905071': 'B.1.1.7', 'OV905072': 'B.1.1.7', 'OV905079': 'B.1.1.7', 'OV905082': 'B.1.1.7', 'OV905085': 'B.1.1.7', 'OV905088': 'B.1.1.7', 'OV905093': 'B.1.1.7', 'OV905098': 'B.1.1.7', 'OV905100': 'B.1.1.7', 'OV905101': 'B.1.1.7', 'OV905102': 'B.1.1.7', 'OV905107': 'B.1.1.7', 'OV905106': 'B.1.1.7', 'OV905108': 'B.1.1.7', 'OV905111': 'B.1.1.7', 'OV905119': 'B.1.1.7', 'OV905120': 'B.1.1.7', 'OV905125': 'B.1.1.7', 'OV905127': 'B.1.1.7', 'OV905129': 'B.1.1.7', 'OV905138': 'B.1.1.7', 'OV905139': 'B.1.1.7', 'OV905140': 'B.1.1.7', 'OV905141': 'B.1.1.7', 'OV905142': 'B.1.1.7', 'OV905143': 'B.1.1.7', 'OV905144': 'B.1.1.7', 'OV905146': 'B.1.1.7', 'OV905153': 'B.1.1.7', 'OV905152': 'B.1.1.7', 'OV905154': 'B.1.1.7', 'OV905157': 'B.1.1.7', 'OV905163': 'B.1.1.7', 'OV905164': 'B.1.1.7', 'OV905162': 'B.1.1.7', 'OV905165': 'B.1.1.7', 'OV905166': 'B.1.1.7', 'OV905169': 'B.1.1.7', 'OV905172': 'B.1.1.7', 'OV905687': 'B.1.1.7', 'OV905689': 'B.1.1.7', 'OV905691': 'B.1.1.7', 'OV905692': 'B.1.1.7', 'OV905696': 'B.1.1.7', 'OV905700': 'B.1.1.7', 'OV905702': 'B.1.1.7', 'OV905703': 'B.1.1.7', 'OV905705': 'B.1.1.7', 'OV905706': 'B.1.1.7', 'OV905707': 'B.1.1.7', 'OV905708': 'B.1.1.7', 'OV905711': 'B.1.1.7', 'OV905715': 'B.1.1.7', 'OV905717': 'B.1.1.7', 'OV905719': 'B.1.1.7', 'OV905722': 'B.1.1.7', 'OV905723': 'B.1.1.7', 'OV905913': 'B.1.1.7', 'OV905923': 'B.1.1.7', 'OV905933': 'B.1.1.7', 'OV905936': 'B.1.1.7', 'OV905940': 'B.1.1.7', 'OV905941': 'B.1.1.7', 'OV906511': 'B.1.1.7', 'OV906515': 'B.1.1.7', 'OV906516': 'B.1.1.7', 'OV906525': 'B.1.1.7', 'OV906526': 'B.1.1.7', 'OV906535': 'B.1.1.7', 'OV906537': 'B.1.1.7', 'OV906539': 'B.1.1.7', 'OV906542': 'B.1.1.7', 'OV906545': 'B.1.1.7', 'OV906548': 'B.1.1.7', 'OV906546': 'B.1.1.7', 'OV906554': 'B.1.1.7', 'OV906558': 'B.1.1.7', 'OW390140': 'B.1.617.2', 'OW390147': 'B.1.617.2', 'OW390192': 'B.1.617.2', 'OW390203': 'B.1.617.2', 'OW390277': 'B.1.617.2', 'OW390299': 'B.1.617.2', 'OW390318': 'B.1.617.2', 'OW390326': 'B.1.617.2', 'OW390348': 'B.1.617.2', 'OW390467': 'B.1.617.2', 'OW390532': 'B.1.617.2', 'OW390668': 'B.1.617.2', 'OW390691': 'B.1.617.2', 'OW390755': 'B.1.617.2', 'OW370259': 'B.1.617.2', 'OW390848': 'B.1.617.2', 'OW390942': 'B.1.617.2', 'OW390993': 'B.1.617.2', 'OW391015': 'B.1.617.2', 'OW391089': 'B.1.617.2', 'OW391095': 'B.1.617.2', 'OW391109': 'B.1.617.2', 'OW391201': 'B.1.617.2', 'OW391565': 'B.1.617.2', 'OW391586': 'B.1.617.2', 'OW391666': 'B.1.617.2', 'OW391712': 'B.1.617.2', 'OW391732': 'B.1.617.2', 'OW371326': 'B.1.617.2', 'OW371330': 'B.1.617.2', 'OW391784': 'B.1.617.2', 'OW371370': 'B.1.617.2', 'OW371388': 'B.1.617.2', 'OW371420': 'B.1.617.2', 'OW391838': 'B.1.617.2', 'OW371464': 'B.1.617.2', 'OW391887': 'B.1.617.2', 'OW371487': 'B.1.617.2', 'OW371526': 'B.1.617.2', 'OW371529': 'B.1.617.2', 'OW371580': 'B.1.617.2', 'OW371586': 'B.1.617.2', 'OW371610': 'B.1.617.2', 'OW371664': 'B.1.617.2', 'OW371675': 'B.1.617.2', 'OW371702': 'B.1.617.2', 'OW392162': 'B.1.617.2', 'OW371767': 'B.1.617.2', 'OW371847': 'B.1.617.2', 'OW392290': 'B.1.617.2', 'OW392555': 'B.1.617.2', 'OW372333': 'B.1.617.2', 'OW392755': 'B.1.617.2', 'OW372367': 'B.1.617.2', 'OW372392': 'B.1.617.2', 'OW372409': 'B.1.617.2', 'OW372473': 'B.1.617.2', 'OW372499': 'B.1.617.2', 'OW372506': 'B.1.617.2', 'OW372515': 'B.1.617.2', 'OW392954': 'B.1.617.2', 'OW372564': 'B.1.617.2', 'OW372565': 'B.1.617.2', 'OW372579': 'B.1.617.2', 'OW392993': 'B.1.617.2', 'OW393005': 'B.1.617.2', 'OW372642': 'B.1.617.2', 'OW372678': 'B.1.617.2', 'OW372719': 'B.1.617.2', 'OW372754': 'B.1.617.2', 'OW372786': 'B.1.617.2', 'OW372812': 'B.1.617.2', 'OW372879': 'B.1.617.2', 'OW372935': 'B.1.617.2', 'OW372951': 'B.1.617.2', 'OW393371': 'B.1.617.2', 'OW372967': 'B.1.617.2', 'OW373035': 'B.1.617.2', 'OW373041': 'B.1.617.2', 'OW373055': 'B.1.617.2', 'OW373069': 'B.1.617.2', 'OW393482': 'B.1.617.2', 'OW373073': 'B.1.617.2', 'OW393499': 'B.1.617.2', 'OW373106': 'B.1.617.2', 'OW373151': 'B.1.617.2', 'OW373162': 'B.1.617.2', 'OW393855': 'B.1.617.2', 'OW373542': 'B.1.617.2', 'OW373545': 'B.1.617.2', 'OW394006': 'B.1.617.2', 'OW373663': 'B.1.617.2', 'OW394053': 'B.1.617.2', 'OW394084': 'B.1.617.2', 'OW394111': 'B.1.617.2', 'OW394135': 'B.1.617.2', 'OW373837': 'B.1.617.2', 'OW373845': 'B.1.617.2', 'OW374028': 'B.1.617.2', 'OW374069': 'B.1.617.2', 'OU356612': 'P.1', 'OU352177': 'P.1', 'OU352265': 'P.1', 'OU356382': 'P.1', 'OU356422': 'P.1', 'OU356928': 'P.1', 'OU351990': 'P.1', 'OU361508': 'P.1', 'OU360555': 'P.1', 'OU326021': 'P.1', 'OU326139': 'P.1', 'MZ454400': 'P.1', 'OU175769': 'P.1', 'OU175786': 'P.1', 'MZ454575': 'P.1', 'MZ454675': 'P.1', 'MZ454903': 'P.1', 'MZ455098': 'P.1', 'MZ455429': 'P.1', 'OU327146': 'P.1', 'MZ368109': 'P.1', 'MZ368110': 'P.1', 'MZ433214': 'P.1', 'MZ384172': 'P.1', 'MZ433487': 'P.1', 'MZ433495': 'P.1', 'MZ279155': 'P.1', 'MZ279200': 'P.1', 'MZ279404': 'P.1', 'MZ433510': 'P.1', 'MZ345911': 'P.1', 'MZ368496': 'P.1', 'MZ368511': 'P.1', 'MZ368518': 'P.1', 'MZ368520': 'P.1', 'MZ368534': 'P.1', 'MZ349105': 'P.1', 'MZ349110': 'P.1', 'MZ433710': 'P.1', 'MZ371124': 'P.1', 'MZ385757': 'P.1', 'MZ385765': 'P.1', 'MZ405567': 'P.1', 'MZ385779': 'P.1', 'MZ433754': 'P.1', 'MZ371176': 'P.1', 'MZ405683': 'P.1', 'MZ385812': 'P.1', 'MZ371199': 'P.1', 'MZ371231': 'P.1', 'MZ371251': 'P.1', 'MZ385883': 'P.1', 'MZ322030': 'P.1', 'MZ322032': 'P.1', 'MZ322033': 'P.1', 'MZ322035': 'P.1', 'MZ322041': 'P.1', 'MZ322069': 'P.1', 'MZ385932': 'P.1', 'MZ385940': 'P.1', 'MZ371324': 'P.1', 'MZ385962': 'P.1', 'MZ371337': 'P.1', 'MZ323709': 'P.1', 'MZ386035': 'P.1', 'MZ386161': 'P.1', 'MZ434171': 'P.1', 'MZ386202': 'P.1', 'MZ371606': 'P.1', 'MZ386214': 'P.1', 'MZ386222': 'P.1', 'MZ434203': 'P.1', 'MZ349675': 'P.1', 'MZ349682': 'P.1', 'OU066644': 'P.1', 'MZ349688': 'P.1', 'MZ434232': 'P.1', 'MZ386275': 'P.1', 'MZ434249': 'P.1', 'MZ349743': 'P.1', 'MZ434290': 'P.1', 'MZ349749': 'P.1', 'MZ386349': 'P.1', 'MZ349760': 'P.1', 'MZ349763': 'P.1', 'MZ411665': 'P.1', 'MZ411660': 'P.1', 'MZ371792': 'P.1', 'MZ349849': 'P.1', 'MZ386417': 'P.1', 'MZ386446': 'P.1', 'MZ328676': 'P.1', 'MZ411833': 'P.1', 'MZ372040': 'P.1', 'MZ411866': 'P.1', 'MZ388144': 'P.1', 'OU061529': 'P.1', 'MZ328971': 'P.1', 'MZ388222': 'P.1', 'OU151076': 'P.1', 'OU352138': 'B.1.351', 'OU356670': 'B.1.351', 'OU352186': 'B.1.351', 'OU340710': 'B.1.351', 'OU342235': 'B.1.351', 'OU342243': 'B.1.351', 'OU342265': 'B.1.351', 'OU356466': 'B.1.351', 'OU342269': 'B.1.351', 'OU356508': 'B.1.351', 'OU361574': 'B.1.351', 'OU139663': 'B.1.351', 'OU326018': 'B.1.351', 'OU326028': 'B.1.351', 'OU124871': 'B.1.351', 'OU124889': 'B.1.351', 'OU175732': 'B.1.351', 'OU175761': 'B.1.351', 'OU175770': 'B.1.351', 'OU125176': 'B.1.351', 'OU175784': 'B.1.351', 'OU125201': 'B.1.351', 'OU180312': 'B.1.351', 'OU140043': 'B.1.351', 'OU116880': 'B.1.351', 'OU125871': 'B.1.351', 'OU125883': 'B.1.351', 'OU290663': 'B.1.351', 'OU126114': 'B.1.351', 'OU176232': 'B.1.351', 'OU176362': 'B.1.351', 'OU176390': 'B.1.351', 'OU117648': 'B.1.351', 'OU117682': 'B.1.351', 'OU126622': 'B.1.351', 'OU126656': 'B.1.351', 'OU176483': 'B.1.351', 'MZ344999': 'B.1.351', 'MZ345001': 'B.1.351', 'OU093684': 'B.1.351', 'MZ368225': 'B.1.351', 'OU106512': 'B.1.351', 'OU094175': 'B.1.351', 'OU106981': 'B.1.351', 'OU106989': 'B.1.351', 'OU107446': 'B.1.351', 'OU107562': 'B.1.351', 'OU066608': 'B.1.351', 'OU095553': 'B.1.351', 'OU066774': 'B.1.351', 'OU072522': 'B.1.351', 'OU096161': 'B.1.351', 'OU083191': 'B.1.351', 'OU083682': 'B.1.351', 'MZ328979': 'B.1.351', 'OU151377': 'B.1.351', 'OU151561': 'B.1.351', 'OU134599': 'B.1.351', 'OU134660': 'B.1.351', 'OU134799': 'B.1.351', 'OU134852': 'B.1.351', 'OU134900': 'B.1.351', 'OU135018': 'B.1.351', 'OU138792': 'B.1.351', 'OU123904': 'B.1.351', 'OU149305': 'B.1.351', 'OU182580': 'B.1.351', 'OU149362': 'B.1.351', 'OU139191': 'B.1.351', 'OU149471': 'B.1.351', 'MZ376663': 'B.1.351', 'OU139260': 'B.1.351', 'OU135800': 'B.1.351', 'OU135995': 'B.1.351', 'OU139382': 'B.1.351', 'OU149773': 'B.1.351', 'OU182806': 'B.1.351', 'OU149846': 'B.1.351', 'OU124688': 'B.1.351', 'OU168626': 'B.1.351', 'OU139634': 'B.1.351', 'OU149968': 'B.1.351', 'OU149976': 'B.1.351', 'OU168731': 'B.1.351', 'OU171203': 'B.1.351', 'OU313807': 'B.1.351', 'OU129298': 'B.1.351', 'OU161090': 'B.1.351', 'OU314387': 'B.1.351', 'MZ341810': 'B.1.351', 'MZ331077': 'B.1.351', 'MZ358462': 'B.1.351', 'MZ394439': 'B.1.351', 'MZ394580': 'B.1.351', 'MZ394616': 'B.1.351', 'MZ394617': 'B.1.351', 'MZ342413': 'B.1.351', 'MZ450533': 'B.1.351', 'MZ378667': 'B.1.351', 'OU070204': 'B.1.351'}\n"
     ]
    }
   ],
   "source": [
    "# Caminho do arquivo de anotações\n",
    "arquivo_anotacoes = '/home/m_souza/tcc/dataset/dataset_agua/anotacoes.txt'\n",
    "\n",
    "# Dicionário para armazenar as anotações\n",
    "annot = {}\n",
    "\n",
    "# Ler o arquivo de anotações\n",
    "with open(arquivo_anotacoes, 'r') as anotacoes_file:\n",
    "    for linha in anotacoes_file:\n",
    "        # Dividir a linha usando a vírgula como separador\n",
    "        partes = linha.strip().split(',') # primeira parte ID longo terminando en ACCN,\n",
    "                                            # segunda parte o tipo atribuido pelo anotador externo\n",
    "        # Verificar se a linha tem o formato correto\n",
    "        if len(partes) == 2:\n",
    "            cabecalho, anotacao = partes\n",
    "            annot[cabecalho.split(\"_\")[1]] = anotacao # new 05.10.2023 / 20/10/2023 Alterado o formato do arquivo de anotações\n",
    "        else:\n",
    "            print(f\"A linha '{linha}' não possui o formato esperado.\")\n",
    "\n",
    "# Exibir o dicionário de anotações\n",
    "nAnnotSeq = len(annot)\n",
    "if nAnnotSeq!=Nseq:\n",
    "    print(\"PROBLEM: THE NUMBER OF FILTERED SEQUENCES AND ANNOTATED SEQUENCES DOES NOT MATCH ...CALL DIEGO!\")\n",
    "else:\n",
    "    print(\"we read \",nAnnotSeq,\" annotated sequence accessions\\n\", annot) # {accesion: Pangolin genotype}\n",
    "\n",
    "# check - all input sequence must be annotated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e065fefa",
   "metadata": {},
   "source": [
    "### IDENTIFYING DIFFERENT ANNOTATED SPECIES (SPECIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3637d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pegando as espécies virais anotadas\n",
    "species=list(set(annot.values()))\n",
    "Nspecies = len(species)\n",
    "print(\"we found \",Nspecies,\" annotated genotypes\\n\", species)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c702b608",
   "metadata": {},
   "source": [
    "### funções que constroem a matriz de distribuição e calculam a resolução do agrupamento usando o ground truth (supervisão para ajuste do hyper-parâmetnro \"repulsion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17b2adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funções que constroem a matriz de distribuição e calculam o erro usando o ground truth\n",
    "\n",
    "# CompareResultAndAnnotation retona a \"distr_matrix\" e \"annotation\"\n",
    "# OS CLUSTERS SÃO ANOTADOS COM 3 CLASSES:\n",
    "# \"EMPTY CLUSTER\", quando está vazio,\n",
    "# \"MULTIPLE SPECIES\" quando 2 ou mais espécies tem o mesmo numero de sequencias no cluster e não\n",
    "# é possível definir a espécie (pre)dominante no cluster, e\n",
    "# \"NOME DA ESPECIE DOMINANTE\", quando uma única espécie é a mais frequente no cluster\n",
    "# (o que não significa que seja a única...)\n",
    "\n",
    "def CompareResultAndAnnotation(accession, annot, clope, verbose=False):\n",
    "\n",
    "    # REQUIREMENT: accession and annot must be compatible in size and ordering !!!\n",
    "\n",
    "    # pegamos a lista de genotipos anotados distintos\n",
    "    # <= numero de sequencias no data set de treinamento\n",
    "\n",
    "    # definimos o nome das colunas da matriz de distribuição\n",
    "    species = list(set(annot))\n",
    "\n",
    "#     if verbose:\n",
    "#         print(\"list of genotypes in dataset:\\n\",species)# numeramos os genótipos segundo a ordem na lista\n",
    "\n",
    "    # definimos o n umero de colunas\n",
    "    nspecies = len(species)\n",
    "\n",
    "    # criamos um dict com a anotacao da especie para atribuir um ordinal à cada especie anotada\n",
    "    species_nmb = {}\n",
    "    # atribuindo um consecutivo a cada genotipo\n",
    "    ctr = 0\n",
    "    for sp in species:\n",
    "        species_nmb[sp] = ctr\n",
    "        ctr+=1\n",
    "\n",
    "    # calculamos o numero de transacoes de cada espécie anotada (a multiplicidade delas)\n",
    "    colsum = np.zeros(nspecies,dtype=int)\n",
    "    for sp in annot:\n",
    "        colsum[species_nmb[sp]]+=1\n",
    "\n",
    "#     if verbose:\n",
    "#         print(\"number of strain by genotype (colsum) \\n\",colsum)\n",
    "\n",
    "    # inicializa a matriz de distribuição\n",
    "    K = clope.max_cluster_number\n",
    "    distr_matrix = np.zeros((K,nspecies),dtype=int)\n",
    "\n",
    "    # constroi a matriz de distribuição\n",
    "    # varre dict transaction {trans_id:cluster}\n",
    "    for trans_id in range(len(clope.transaction)): # trans_id =  clope key\n",
    "        cluster = clope.transaction[trans_id] #\n",
    "        distr_matrix[cluster,species_nmb[annot[trans_id]]] += 1 # OK\n",
    "\n",
    "    # attributing most probable genotype to each cluster\n",
    "    annotation = {}\n",
    "    for cluster in range(K):\n",
    "        if np.sum(distr_matrix[cluster])>0:\n",
    "            # anotando cada cluster com a especie mais provável\n",
    "            sort = -np.sort(-distr_matrix[cluster]) # ORDENANDO DE MAIOR A MENOR\n",
    "            if sort[1]==sort[0]: # tem mais de um máximo\n",
    "                annotation[cluster]=\"MULTIPLE SPECIES\" # accession[cluster] is not good\n",
    "                # \"UNKNOWN\"  TWO OR MORE SPECIS WITH EQUAL FREQUENCY\n",
    "            else:\n",
    "                annotation[cluster]=species[np.argmax(distr_matrix[cluster])]\n",
    "        else:\n",
    "            annotation[cluster]=\"EMPTY CLUSTER\" # CLUSTER VAZIO - RARIDADE MAS NÃO IMPOSSIVEL\n",
    "\n",
    "    return np.array(distr_matrix), annotation\n",
    "\n",
    "def GetClusteringResolution(distrmat):\n",
    "\n",
    "    # cálculo da métrica de erro relativo por cluster:\n",
    "    # metric = indice_agrupamento / indice_dispersão\n",
    "    # indice_agrupamento = nseqs_of_more_freq_species/nmb_of_seqs/nclust # <=1\n",
    "    # indice_dispersão = Nspecies - not_present_species # >=1\n",
    "\n",
    "    resolution=[]\n",
    "    n = np.sum(distrmat) # numero total de sequencias\n",
    "    dispersed_seqs = 0\n",
    "    Nspecies = distrmat.shape[1]\n",
    "    nclust = distrmat.shape[0]\n",
    "    for cluster in range(nclust): # varre os clusters clope\n",
    "        row = list(distrmat[cluster]) # pega a linha da matriz de distribuição do cluster,\n",
    "        # que diz qtas sequencias de cada espécies estão representadas no cluster\n",
    "        not_present_species = row.count(0) # conta qtas espécies não tem representante no cluster\n",
    "        nmb_of_seqs = np.sum(row) # number of seqs in this cluster\n",
    "        nseqs_of_more_freq_species = np.max(row)# numero de seqs da espécie mais frequente no cluster\n",
    "        dispersed_seqs += nmb_of_seqs - nseqs_of_more_freq_species # number of seqs outside the most frequented cluster\n",
    "        if nmb_of_seqs > 0:\n",
    "            indice_agrupamento = nseqs_of_more_freq_species/nmb_of_seqs/nclust**0.5 #<=1\n",
    "            indice_dispersão = Nspecies - not_present_species # >=1\n",
    "            resolution.append( indice_agrupamento / indice_dispersão )\n",
    "        else:\n",
    "            resolution.append(0) # EMPTY CLUSTER CASE ------------->\n",
    "\n",
    "    return np.array(resolution), 1 - dispersed_seqs/n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a1c0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accession(new_accession, transactions):\n",
    "    for j in range(len(transactions)):\n",
    "#         print(new_accession,\"=?\",transactions[j][0].replace(\"/\",\"\"))\n",
    "        if new_accession in transactions[j][0].replace(\"/\",\"\"):\n",
    "            return transactions[j][0]\n",
    "    return \"unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22db85a3",
   "metadata": {},
   "source": [
    "## OPTIMIZATION PROCESS OF THE UNSUPERVISED\n",
    "## CLUSTERING ALGORITHM HYPER-PARAMETER\n",
    "### it is done by:\n",
    "#### 1. varying repulsion in a given search interval\n",
    "#### 2. doing several random iterations of CLOPE clustering for each repulsion sorting input data\n",
    "#### 3. recording the clustering that maximizes our quality criteria which is based on less\n",
    "####  dispersion of annotated especies among the found clusters\n",
    "### >>>> A quality threshold is defined as an early stopping trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff438b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# MAIN -----------------------------------------------------\n",
    "\n",
    "# pegando a lista de accession\n",
    "Acc = gvf['accession']\n",
    "# tirando barras para match com annot\n",
    "Accession0=[]\n",
    "for acc in Acc:\n",
    "    Accession0.append(acc.replace(\"/\",\"\")) # um dataset de Inês tinha \"/\"\n",
    "# print(Accession0[:3])\n",
    "\n",
    "# pegando o groundthruth da anotação\n",
    "GroundTruth0 = list(annot.values())\n",
    "#GroundTruth0[:3]\n",
    "\n",
    "# pegando os vetores de chars dos códigos das classes primárias como transações\n",
    "InputData0 = pvec\n",
    "nmb_trans=len(pvec)\n",
    "\n",
    "nmb_annot=len(GroundTruth0)\n",
    "print('check: number of transactions = annotations: ',nmb_trans,'=',nmb_annot)\n",
    "\n",
    "if nmb_trans!=nmb_annot:\n",
    "    print(\"DATASET NOT FULLY OR WRONGLY ANNOTATED .....\\nRESULTS MAY BE CONFUSING....\")\n",
    "\n",
    "noiseLimit = 0.0 # dummy\n",
    "\n",
    "nRandIterations = 30 # 50 equivalent to bootstrap\n",
    "ClusterResolutionThres = 0.5 #1.00\n",
    "\n",
    "print(\"Looking for optimal repulsion with \",nRandIterations,\\\n",
    "      \"random iterations and threshold of minimum cluster resolution of \",ClusterResolutionThres)\n",
    "\n",
    "optimum=False\n",
    "minRep = 1\n",
    "maxRep = 16\n",
    "stpRep = 2\n",
    "\n",
    "trans_len=len(InputData0[0])\n",
    "\n",
    "best_repulsion = -1\n",
    "best_resolution = 0\n",
    "K = nmb_trans\n",
    "\n",
    "# BUSCANDO PELA MELHOR REPULSION ACOMPANHANDO NOSSA METRICA DE AGRUPAMENTO\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "for repulsion in range(minRep,maxRep,stpRep): #>= 11 da 1005 acc de clusters com 200 iteracoes randomicas\n",
    "\n",
    "    print(\"\\n>>>> repulsion\", repulsion)\n",
    "\n",
    "    cluster_resolution=[]\n",
    "    Resolution=[]\n",
    "\n",
    "    best_K = nmb_trans\n",
    "\n",
    "    codes = InputData0\n",
    "    GT = GroundTruth0\n",
    "\n",
    "    # EXECUTANDO nRandIterations AGRUPAMENTOS COM CLOPE (NÃO SUPERVIDIONADO)\n",
    "    # COM O VALOR DE REPULSION DADO, OLHANDO SEMPRE PARA O MELHOR AGRUPAMENTO\n",
    "\n",
    "    for i in range(nRandIterations):\n",
    "\n",
    "        seed = None #150 <<<<<<<<<<<<<<<<<<<<<<<< ORDENAÇÂO ALEATORIA\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # not needed because clope shuffle input data every it is called << NEW 07.10.2023\n",
    "    #     InputData=InputData0.copy()\n",
    "#         list_zipped = list(zip(Accession0,InputData0,GroundTruth0))\n",
    "#         np.random.shuffle(list_zipped)\n",
    "#         Accession, InputData, GroundTruth = zip(*list_zipped) #unzipping\n",
    "\n",
    "        # transferimos os dados originais para as vari[aveis de trabalho\n",
    "\n",
    "        Accession = Accession0.copy()\n",
    "        InputData = InputData0.copy()\n",
    "        GroundTruth = GroundTruth0.copy()\n",
    "\n",
    "        # print('first transactions:\\n',InputData[:3])\n",
    "        # print('first annotations:\\n',GroundTruth[:3])\n",
    "\n",
    "        # criamos o dataset formato CLOPE ordeando de forma aleatoria os dados de entrada\n",
    "\n",
    "        Data = {} # usamos um dicionario para os dados com as features indexadas por posição\n",
    "\n",
    "        for trans_nmb in range(0, len(InputData)):\n",
    "            Data[trans_nmb] = [''] * trans_len # inicializa a transação efetiva\n",
    "            fst = 0 # for SARS2\n",
    "            missing = []\n",
    "            for index in range(fst, len(InputData[trans_nmb])):\n",
    "                # adicionando a posição (coluna) da feature se não for missing\n",
    "                if InputData[trans_nmb][index] != '?':\n",
    "                    Data[trans_nmb][index] = str(index) + \":\"+InputData[trans_nmb][index].replace(\" \",\"\")  # add feature position\n",
    "                else: # counting missings\n",
    "                    missing.append(index)\n",
    "                if len(missing)>0:\n",
    "                    print(\"WARNING: there were \",len(missing),\" missing features in transaction \",\\\n",
    "                          trans_nmb,\" at positions:\\n\",missing)\n",
    "\n",
    "        # criamos um novo clusterizador\n",
    "        clope = CLOPE(print_step=1000, random_seed=seed)\n",
    "\n",
    "        # Dados iniciais # Inicializamos o algoritmo\n",
    "        clope.init_clusters(Data, repulsion, noiseLimit)\n",
    "\n",
    "        # Iteramos até o metodo de clusterização não supervisionado convergir\n",
    "        ctr=0\n",
    "        while clope.next_step(Data, repulsion, noiseLimit ) > 0:\n",
    "            ctr+=1\n",
    "            clope.print_history_count(repulsion, seed)\n",
    "\n",
    "        print(\"RANDOM ITERATION \",i+1,\" -> clope iterations: \",ctr,\" number of clusters: \",clope.max_cluster_number)\n",
    "\n",
    "#         print(\"CLOPE number of clusters \",clope.max_cluster_number)\n",
    "        # print(clope.transaction.values())\n",
    "        # print(\"annot\",list(GroundTruth))\n",
    "\n",
    "        # PARTE SUPERVISIONADA ================================================================\n",
    "\n",
    "        # construção da matriz de distribuição: numero de cada genotipo (colunas) em cada cluster clope\n",
    "        # (linhas)\n",
    "\n",
    "        distrmat, map_species2cluster = CompareResultAndAnnotation(Accession, GroundTruth, clope,True)\n",
    "\n",
    "        # cálculo da \"resolução da classificação\" ( novo conceito nosso: a resolução aumenta\n",
    "        # na medida que os clusters são mais homogêneos no sentido de conter um mesmo genótipo\n",
    "        # preferencialmente )\n",
    "        # A maior homogeneidade é qdo todas as sequencias de um cluster clope pertencem ao mesmo\n",
    "        # genotipo. não importa que vários clusters clope contenha esse mesmo genótipo. o que conta\n",
    "        # é a homegeneidade media * homgeneidade minima dos clusters\n",
    "\n",
    "        resolution_lst, tot_resolution = GetClusteringResolution(distrmat)\n",
    "\n",
    "        cluster_resolution.append(resolution_lst)\n",
    "        Resolution.append(tot_resolution)\n",
    "\n",
    "        # condition to choose the best clustering during iterations USING MIN*MEAN Resolution\n",
    "        resol = np.sqrt(np.min(cluster_resolution[-1])*np.mean(cluster_resolution[-1]))\n",
    "\n",
    "        # GUARDA AS INFORMAÇÕES DA MELHOR CLUSTERIZAÇÃO ATÉ O MOMENTO\n",
    "        if ( resol > best_resolution): #\n",
    "        # and (len(distrmat) < best_K): CRITERIO QUE USEI SEM MUITO CRITÉRIO , KKKKKKKK\n",
    "            # A CONDIÇÃO ACIMA BUSCA A COMBINAÇÃO COM MENOR DISTRIBUIÇÃO (MAIOR ACURACIA)\n",
    "            # E MENOR NUMERO DE CLUSTERS !!!!!! -->>>>>> tirei essa condição 18.10.2023\n",
    "            best_clusters_resolution = cluster_resolution[-1]\n",
    "            best_resolution = resol # or mean USING MIN Resolution\n",
    "            best_K = len(distrmat)\n",
    "            K = best_K\n",
    "            best_distrmat = distrmat\n",
    "            best_annotation = map_species2cluster\n",
    "            map_class2cluster = clope.transaction\n",
    "            codes = InputData\n",
    "            GT = GroundTruth\n",
    "            best_accession = Accession\n",
    "            best_repulsion = repulsion\n",
    "            print(\">>>>>>>>>>>>>>>> best found with \",best_K,\" clusters. resolution = \",best_resolution)\n",
    "\n",
    "        del clope # deleta o classificador - economia de memoria\n",
    "\n",
    "    # controle de optimalidade\n",
    "    # AQUI SE DEFINE QUÃO EXIGENTE É O ESTUDO, SE PERMITE ALGUM NIVEL DE DISTRIBUIÇÃO OU NÃO\n",
    "    # SE  ClusterResolutionThres = 1 => É PORQUE NÃO HOUVE NENHUMA MSITURA DE ESPECIES NOS CLUSTERS, OU SEJA\n",
    "    # CADA CLUSTER CONTINHA UM UNICO TIPO DE ESPECIE ANOTADA\n",
    "    if best_resolution >= ClusterResolutionThres: # NEW 08.10.2023\n",
    "        print(\"satisfied optimality criteria\")\n",
    "        optimum=True\n",
    "        break # saimos do loop aumentando repulsion\n",
    "\n",
    "print(\"OPTIMIZATION ENDED\\nNumber of repulsions tested: \",int((maxRep-minRep)/stpRep),\\\n",
    "      \"\\nNumber of random iterations: \",nRandIterations,\"\\nElapsed time: \",(time.time()-tic)/60,\" min\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dc664a",
   "metadata": {},
   "source": [
    "## PREENCHENDO O MODELO COM A CLUSTERIZAÇÃO ÓTIMA (MELHOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e9bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MeanResolution(x,w):\n",
    "    WM = []\n",
    "    n = len(x)\n",
    "    if len(w) != n:\n",
    "        print(\"WRONG WEIGHTS ----------------------------------------------\")\n",
    "        return -1\n",
    "    m = np.sum(w)\n",
    "    for i in range(n):\n",
    "        WM.append(x[i] * w[i]/m)\n",
    "    return WM\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "if optimum:\n",
    "    print(\"optimum criterium found for repulsion = \",best_repulsion,\" (min*mean)^(1/2) resolution = \",best_resolution)\n",
    "else:\n",
    "    print(\"optimum not found for repulsion in the interval [\",minRep,\",\",maxRep,\"] best (min*mean)^(1/2) resolution = \",best_resolution)\n",
    "\n",
    "# evaluate clustering resolution\n",
    "x = best_clusters_resolution\n",
    "w = np.sum(best_distrmat, axis=1)\n",
    "\n",
    "print(\"Check\", len(x),len(w))\n",
    "\n",
    "wr = MeanResolution(x,w)\n",
    "\n",
    "model.NmbOfClusters = K\n",
    "model.Resolution = wr # mean\n",
    "model.MinClusterResolution = np.min(wr) # MODEL\n",
    "model.DistributionMatrix = best_distrmat # MODEL\n",
    "model.CodeOfClass = codes # origin CBUC but randomly shuffled by AGUA\n",
    "model.GroundTruth = GT # ANNOTATION\n",
    "model.TheClusterOfClass = map_class2cluster # CLOPE\n",
    "model.TheSpeciesOfCluster = list(best_annotation.values()) # CLOPE + ANNOTATION\n",
    "model.CLOPE_repulsion = best_repulsion # MODEL\n",
    "baccession=[]\n",
    "for acc in best_accession:\n",
    "    baccession.append(get_accession(acc,t))\n",
    "model.Accession = baccession # CBUC but shuffled by AGUA\n",
    "\n",
    "print(\"Mean resolution of the model \",np.sum(model.Resolution),\\\n",
    "      \" standard deviation of resolution \",np.std(model.Resolution))\n",
    "print(\"Number of Classes :\",model.NmbOfClasses)\n",
    "#     print(\"CodesOf Classes :\\n\",model.CodeOfClass)\n",
    "#     print(\"Annotation of Classes :\\n\",model.GroundTruth)\n",
    "print(\"Number of clusters :\",model.NmbOfClusters)\n",
    "df_distrmat_SpeciesByClusters = pd.DataFrame(zip(model.DistributionMatrix,model.TheSpeciesOfCluster))\n",
    "print(\"Confusion matrix\\n\",df_distrmat_SpeciesByClusters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e49c2c1",
   "metadata": {},
   "source": [
    "## visualizing clusters with multiple species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118fed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing clusters with multiple species\n",
    "\n",
    "# ESSES CLUTERS DEVEM CONTER SEQUENCIAS QUE NÃO FORAM BEM CLASSIFICADAS ...\n",
    "\n",
    "mixed_clusters_nmb = 0\n",
    "for i in range(len(df_distrmat_SpeciesByClusters[1])):\n",
    "    if df_distrmat_SpeciesByClusters[1][i]==\"MULTIPLE SPECIES\":\n",
    "        mixed_clusters_nmb += 1\n",
    "        print(i,df_distrmat_SpeciesByClusters[0][i],\" nseqs (\",np.sum(df_distrmat_SpeciesByClusters[0][i]),\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551a2a28",
   "metadata": {},
   "source": [
    "# FASE2 - CÁLCULO DA MATRIZ DE DISTÂNCIAS ENTRE PARES DE CLASSES PRIMÁRIAS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d0a095",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculating distances between pairs of IDs -> put in categorical distance matrix\n",
    "\n",
    "def categoricaldist(u,v,var,mode=0): # calcula a distancia entre 2 vetores categóricos:\n",
    "    # numero de entradas distintas / total de entradas\n",
    "    dist=0\n",
    "    for i in range(len(u)):\n",
    "        if u[i]!=v[i]:\n",
    "            if mode==0:\n",
    "                dist+=1/var[i] # weighted -> INVERSAMENTE PROPORCIONAL À VARIABILIDADE DO SITE\n",
    "            else:\n",
    "                dist+=1 # SEM CONSIDERAR A VARIABILIDADE DO SITE\n",
    "\n",
    "    return dist/len(u)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c245828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "\n",
    "print(\"FASE2 - CÁLCULO DA MATRIZ DE DISTÂNCIAS ENTRE PARES DE CLASSES PRIMÁRIAS \")\n",
    "\n",
    "fvec=model.CodeOfClass\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "cat_distmatrix=[]\n",
    "for i in range(P-1):\n",
    "    cat_distmatrix.append([])\n",
    "    for j in range(i+1,P):\n",
    "#         dotdistmatrix[-1].append(np.dot(pvec[i],pvec[j]))\n",
    "        cat_distmatrix[-1].append(categoricaldist(fvec[i],fvec[j],var,0))  # calling weighted\n",
    "        # lçast prm above: calling in mode=0 sem pesos, calling in mode=1 com pesos\n",
    "#         print(i,j,np.dot(pvec[i],pvec[j]))\n",
    "\n",
    "# check\n",
    "# for distvec in cat_distmatrix:\n",
    "#     print(distvec)\n",
    "\n",
    "# gerando matriz de distancias simetrica\n",
    "\n",
    "# nota: os vetores DistVec , fst e scd estão alinhados, ou seja DistVec[k] é a distância entre a classe fst[k] e a scd[k] !!!\n",
    "\n",
    "DistMat=np.zeros((P,P),dtype=float)\n",
    "labels=[]\n",
    "# fst & scd são o par de indexadores de cada distância (as classes primárias que são comparadas)\n",
    "#atualizando labels com a especie predita\n",
    "fst=[]\n",
    "scd=[]\n",
    "for i in range(P-1):\n",
    "    labels.append(\"pclass:\"+str(i)+\\\n",
    "                  \"|pacc:\"+pAcc[i]+\\\n",
    "                  \"|cluster:\"+str(model.TheClusterOfClass[i])+\\\n",
    "                  \"|annot:\"+model.GroundTruth[i]+\\\n",
    "                  \"|pred:\"+model.TheSpeciesOfCluster[model.TheClusterOfClass[i]])\n",
    "    for j in range(i+1,P):\n",
    "        fst.append(i)\n",
    "        scd.append(j)\n",
    "        DistMat[i][j] = cat_distmatrix[i][j-i-1]\n",
    "        DistMat[j][i] = DistMat[i][j] # building a symmetric matrix\n",
    "\n",
    "# labeling last row\n",
    "labels.append(\"pclass:\"+str(P-1)+\\\n",
    "              \"|pacc:\"+pAcc[P-1]+\\\n",
    "              \"|cluster:\"+str(model.TheClusterOfClass[P-1])+\\\n",
    "              \"|annot:\"+model.GroundTruth[P-1]+\\\n",
    "              \"|pred:\"+model.TheSpeciesOfCluster[model.TheClusterOfClass[P-1]])\n",
    "\n",
    "print(\"matriz de distâncias\\n\", DistMat)\n",
    "\n",
    "# print(DistMat.shape)\n",
    "\n",
    "# CONSTRUINDO MATRIZ DE DISTANCIAS LOW TRIANGULAR E VETOR DE LABELS PARA GERAR ARVORE NEWICK\n",
    "LowTriang = []\n",
    "LowLabels = []\n",
    "matsz=len(DistMat)\n",
    "for i in range(matsz-1,-2,-1):\n",
    "    row=[]\n",
    "    for j in range(i+1,matsz):\n",
    "#         print(i,j)\n",
    "        row.append(DistMat[i,j])\n",
    "    if len(row)>0:\n",
    "        LowTriang.append(row)\n",
    "        LowLabels.append(pAcc[i])\n",
    "\n",
    "print(\"number of samples in the distance matrix: \",len(LowLabels),\"\\nLabels: \",LowLabels)\n",
    "\n",
    "# END LOW TRIANGULAR WORK\n",
    "\n",
    "# SALVA A MATRIZ DE DISTANCIAS\n",
    "np.savetxt(model.name+'_dist_mat.txt', DistMat)\n",
    "\n",
    "DistVec = squareform(DistMat) # saida para a construcao de arvores com dendogram\n",
    "print(\"distance vector:\\n\", DistVec)\n",
    "\n",
    "print(f\"tempo:{time.time()-start} segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e531e8",
   "metadata": {},
   "source": [
    "# FASE 3 - ANÁLISE QUALITATIVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320de84a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# vendo a composição detalhada de cada cluster 'clope'\n",
    "# print(model.TheClusterOfClass)\n",
    "Label4Tree={}\n",
    "for target in range(model.NmbOfClusters):\n",
    "    print(\">>>>>>>> cluster \",target,\" contains sequences of the following \",\\\n",
    "          np.sum(model.DistributionMatrix[target]),\" genotypes:\")\n",
    "    for pclass in model.TheClusterOfClass.keys():\n",
    "        if model.TheClusterOfClass[pclass]==target:\n",
    "#             indice = pClass.index(pclass)\n",
    "#             accession_correspondente = pAcc[indice]\n",
    "            print(\"primary class \",pclass,\" with accession \",pAcc[pClass.index(pclass)],\\\n",
    "                  \" annotated as \",model.GroundTruth[pclass])\n",
    "            Label4Tree[pAcc[pClass.index(pclass)]]=pAcc[pClass.index(pclass)]+\":\"+str(target)+\":\"+model.GroundTruth[pclass]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c71ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "Label4Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db316eb6",
   "metadata": {},
   "source": [
    "# visualizando árvore filogenética"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4630f606",
   "metadata": {},
   "source": [
    "# linkage methods\n",
    "The following are methods for calculating the distance between the newly formed cluster  and each .\n",
    "\n",
    "1. method=’single’  also known as the Nearest Point Algorithm.\n",
    "\n",
    "2. method=’complete’  also known by the Farthest Point Algorithm or Voor Hees Algorithm.\n",
    "\n",
    "3. method=’average’ also called the UPGMA algorithm.\n",
    "\n",
    "4. method=’weighted’ also called WPGMA.\n",
    "\n",
    "5. method=’centroid’  also known as the UPGMC algorithm.\n",
    "\n",
    "6. method=’median’  also known as the WPGMC algorithm.\n",
    "\n",
    "7. method=’ward’ also known as the incremental algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98084e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods=['single', 'complete','average','weighted','centroid','median','ward']\n",
    "# methodknownas=['NearestPoint', 'FarthestPoint','UPGMA','WPGMA','UPGMC','WPGMC','Incremental']\n",
    "methods=['average']\n",
    "methodknownas=['UPGMA']\n",
    "# methods=['ward','average','weighted']\n",
    "# methodknownas=['Incremental','UPGMA','WPGMA']\n",
    "nmeth=len(methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5d58eb",
   "metadata": {},
   "source": [
    "# construindo arvores com distintos métodos de linkagem da scipy.cluster.hierarch.linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da3f7ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Função para converter um ClusterNode em Newick\n",
    "def get_newick(node, labels):\n",
    "    if node.is_leaf():\n",
    "        return labels[node.id]\n",
    "    left = get_newick(node.left, labels)\n",
    "    right = get_newick(node.right, labels)\n",
    "    return f\"({left},{right})\"\n",
    "\n",
    "# Função personalizada para obter a representação Newick com comprimento dos ramos\n",
    "def get_newick_with_branch_length(node, lista_de_nomes):\n",
    "    if node.is_leaf():\n",
    "        return f\"{lista_de_nomes[node.id]}:{node.dist}\"\n",
    "    else:\n",
    "        esquerda = get_newick_with_branch_length(node.left, lista_de_nomes)\n",
    "        direita = get_newick_with_branch_length(node.right, lista_de_nomes)\n",
    "        return f\"({esquerda},{direita}):{node.dist}\"\n",
    "\n",
    "# construindo arvores com distintos métodos de linkagem da scipy.cluster.hierarch.linkage\n",
    "# mostrando onde os accessions foram parar sem analisar nem comparar\n",
    "# para usar com tree_analyzer !!!!\n",
    "ctr=0\n",
    "\n",
    "working_labels = labels.copy()\n",
    "short_labels = [label.split('|pacc:')[1] for label in working_labels]\n",
    "short_labels = [label.split('|')[0] for label in short_labels]\n",
    "\n",
    "# print(\"old: \\n\",short_labels)\n",
    "\n",
    "nshort_labels=[]\n",
    "for i in range(len(short_labels)):\n",
    "    nshort_labels.append(Label4Tree[short_labels[i]])\n",
    "\n",
    "for meth in methods:\n",
    "    linkage_matrix = linkage(DistVec, meth)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 18))\n",
    "    fig.set_size_inches(10, 18, forward=True)\n",
    "\n",
    "    dn = dendrogram(linkage_matrix, ax=ax, labels=nshort_labels, \\\n",
    "                    show_leaf_counts=True, show_contracted=True, \\\n",
    "                    orientation='left',leaf_font_size=5)\n",
    "    plt.title(\"SARS2 - Host independent study with linkage method '\"+meth+\"' (\"+methodknownas[ctr]+\")\")\n",
    "\n",
    "    plt.tight_layout(pad=4)\n",
    "    plt.show()\n",
    "    tree_name = 'tree_'+methodknownas[ctr]\n",
    "    fig.savefig(tree_name+'.png', dpi=fig.dpi)\n",
    "\n",
    "    ctr+=1\n",
    "\n",
    "\n",
    "# MAURICIO - AQUI TEM QUE DAR UM JEITO DE SALVAR O DENDOGRAMA OU A LINKAGE MATRIX EM FORMATO NEWICK\n",
    "\n",
    "    # E DEPOIS LER O ARQUIVO E PLOTAR PAR CONFERIR QUE FUNCIONA\n",
    "\n",
    "#     # Converter a matriz de ligação em uma árvore hierárquica\n",
    "#     tree = to_tree(linkage_matrix, rd=False)\n",
    "\n",
    "#     # Salvar a árvore no formato Newick\n",
    "#     with open(tree_name + '.newick', 'w') as f:\n",
    "#         newick = get_newick(tree, nshort_labels)\n",
    "#         f.write(newick + \";\")\n",
    "\n",
    "#     # Carregar a árvore a partir do arquivo Newick\n",
    "#     tree = Tree(tree_name + '.newick', quoted_node_names=True)\n",
    "\n",
    "#     # Criar um estilo para a plotagem da árvore (opcional)\n",
    "#     ts = TreeStyle()\n",
    "#     ts.show_leaf_name = True\n",
    "#     ts.show_branch_length = True\n",
    "\n",
    "#     # Plotar a árvore\n",
    "#     tree.render(tree_name + \".png\", tree_style=ts)\n",
    "\n",
    "print(\"END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc90911b",
   "metadata": {},
   "source": [
    "# CRIANDO E EXPORTANDO ARVORE NEWICK USANDO UPGMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5b3de4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# MAURICIO - AQUI TENTEI DESSA FORMA MAS VEJA QUE A ARVORE ESTA CORROMPIDA, NAO SERVE\n",
    "\n",
    "# CRIANDO E EXPORTANDO ARVORE NEWICK USANDO UPGMA\n",
    "\n",
    "plt.rc('font', size=10)  # Substitua 12 pelo tamanho desejado\n",
    "\n",
    "nLowLabels =[]\n",
    "for i in range(len(LowLabels)):\n",
    "    nLowLabels.append(Label4Tree[LowLabels[i]])\n",
    "\n",
    "# Crie uma matriz de distâncias Biopython\n",
    "dm = DistanceMatrix(nLowLabels, LowTriang)\n",
    "\n",
    "# Use o construtor de árvore de distância UPGMA\n",
    "constructor = DistanceTreeConstructor()\n",
    "# tree_NJ = constructor.nj(dm)  # Substitua por 'nj' para usar o método Neighbor Joining\n",
    "tree_UPGMA = constructor.upgma(dm)  # Substitua por 'nj' para usar o método Neighbor Joining\n",
    "\n",
    "# Salve a árvore em formato Newick\n",
    "# with open('arvore_NJ.nwk', 'w', encoding='utf-8') as f:\n",
    "#     Phylo.write(tree_NJ, f, 'newick')\n",
    "\n",
    "with open('arvore_UPGMA.nwk', 'w', encoding='utf-8') as f:\n",
    "    Phylo.write(tree_UPGMA, f, 'newick')\n",
    "# Plote a árvore (opcional)\n",
    "# Crie uma figura com tamanho personalizado\n",
    "\n",
    "# Crie uma figura com tamanho personalizado\n",
    "fig, ax = plt.subplots(figsize=(18, 55))  # Ajuste o tamanho conforme desejado\n",
    "\n",
    "# Plote a árvore\n",
    "# Phylo.draw(tree_NJ, axes=ax, label_func=lambda x: '' if not x.is_terminal() else x.name)\n",
    "Phylo.draw(tree_UPGMA, axes=ax, label_func=lambda x: '' if not x.is_terminal() else x.name)\n",
    "# Exiba o gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc59636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model(model,n):\n",
    "    print(\"name:\", model.name)\n",
    "    print(\"ListOfVarSites:\\n\",model.ListOfVarSites)\n",
    "    print(\"NmbOfClasses:\",model.NmbOfClasses)\n",
    "    print(\"CodeOfClass[:\",n,\"]:\\n\",model.CodeOfClass[:n])\n",
    "    print(\"GroundTruth[:\",n,\"]:\\n\",model.GroundTruth[:n])\n",
    "    print(\"Accession[:\",n,\"]:\\n\",model.Accession[:n])\n",
    "    print(\"repulsion:\",model.repulsion)\n",
    "    print(\"NmbOfClusters:\",model.NmbOfClusters)\n",
    "    print(\"TheSpeciesOfCluster:\\n\",model.TheSpeciesOfCluster)\n",
    "    print(\"MinClusterAcc:\",model.MinClusterResolution)\n",
    "    print(\"Resolution:\",model.Resolution)\n",
    "    print(\"CLOPE repulsion parameter:\",model.CLOPE_repulsion)\n",
    "    print(\"DistributionMatrix:\\n\",model.DistributionMatrix)\n",
    "    print(\"TheClusterOfClass:\\n\",model.TheClusterOfClass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba1a075",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_model(model,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e26f99",
   "metadata": {},
   "source": [
    "# CREATING THE GENE VARIANT FILE\n",
    "## TO DO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8224ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating and saving the final GVF file\n",
    "\n",
    "# GVF=pd.DataFrame(model.CodeOfClass)\n",
    "# col_rename_dict = {i:j for i,j in zip(GVF.columns,model.ListOfVarSites)}\n",
    "# GVF.rename(columns=col_rename_dict, inplace=True)\n",
    "\n",
    "# GVF['accession']=model.Accession\n",
    "# GVF['ground_truth']=model.GroundTruth\n",
    "\n",
    "# GVF\n",
    "\n",
    "# GVF.to_csv(model.name+'.gvf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa8bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(GVF.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dafa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving and loading the  model\n",
    "\n",
    "# Save the file\n",
    "model_filename=\"model.\"+model.name+\".obj\"\n",
    "dill.dump(model, file = open(model_filename, \"wb\"))\n",
    "\n",
    "# Reload the file\n",
    "loaded_model = dill.load(open(model_filename, \"rb\"))\n",
    "print_model(loaded_model,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04157cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READY TO USE IN THE PRODUCTION PHASE - 23.10.2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8531b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
